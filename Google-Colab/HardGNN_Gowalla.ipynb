{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwuQZAcamr7O",
        "outputId": "6b5ff256-dcc8-4a6f-b52d-f0127364c80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current working directory: /content/drive/MyDrive/Google-Colab\n",
            "Files in current directory:\n",
            "fallback_files\n",
            "Datasets\n",
            "__pycache__\n",
            "Utils\n",
            "Experimentation.md\n",
            "DataHandler.py\n",
            "HardGNN_Colab_Script.py\n",
            "HardGNN_Jupyter_Notebook_Guide.md\n",
            "HardGNN_model.py\n",
            "IMPLEMENTATION_SUMMARY.md\n",
            "main.py\n",
            "Params.py\n",
            "README.md\n",
            "VERIFY_COLAB_READY.py\n",
            "requirements_final.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# IMPORTANT: Update this path to where you uploaded the 'Google-Colab' folder!\n",
        "# Example: If you uploaded it to MyDrive -> HardGNN_Project -> Google-Colab\n",
        "# the path would be '/content/drive/MyDrive/HardGNN_Project/Google-Colab'\n",
        "import os\n",
        "project_path = '/content/drive/MyDrive/Google-Colab'\n",
        "os.chdir(project_path)\n",
        "\n",
        "# Verify the current working directory and list files to confirm\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(\"Files in current directory:\")\n",
        "for item in os.listdir('.'):\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Sj1EOXEnDTX"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# HardGNN: Hard Negative Sampling Enhanced SelfGNN for Google Colab\n",
        "# ========================================================================\n",
        "#\n",
        "# This script adds hard negative sampling to the validated SelfGNN configurations.\n",
        "# Copy each section into separate Colab cells as indicated by the comments.\n",
        "#\n",
        "# Configuration: Uses validated parameters + Hard Negative Sampling (τ=0.1, K=5, λ=0.1)\n",
        "# ========================================================================\n",
        "\n",
        "# ========================================================================\n",
        "# Environment Setup and Installation\n",
        "# ========================================================================\n",
        "\n",
        "\"\"\"\n",
        "🚀 HardGNN: Hard Negative Sampling Enhanced SelfGNN\n",
        "\n",
        "This notebook adds hard negative sampling to validated SelfGNN configurations:\n",
        "- Uses proven hyperparameters for each dataset\n",
        "- Adds InfoNCE contrastive loss (τ=0.1, K=5, λ=0.1)\n",
        "- Dataset-agnostic design\n",
        "- GPU acceleration on Google Colab\n",
        "\n",
        "## 📋 Setup Instructions:\n",
        "1. Runtime → Change runtime type → GPU (T4, A100, or V100)\n",
        "2. Set DATASET parameter below to your desired dataset\n",
        "3. Run cells in order - dependencies will be installed automatically\n",
        "4. Monitor training - logs show contrastive loss alongside standard metrics\n",
        "\"\"\"\n",
        "\n",
        "# ========================================================================\n",
        "# 🔧 CONFIGURE YOUR EXPERIMENT HERE\n",
        "# ========================================================================\n",
        "DATASET = 'gowalla'  # Options: 'yelp', 'amazon', 'gowalla', 'movielens'\n",
        "# ========================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K77czSaqp0FS",
        "outputId": "842dc8b8-9dc0-4da0-b490-1b51520da7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "🚀 HardGNN Setup for Google Colab Pro+ (PRIORITIZING COLAB DEFAULTS for TF/NumPy)\n",
            "============================================================\n",
            "🔄 Installing/upgrading non-ML core dependencies for HardGNN...\n",
            "📍 Detected Python version: 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n",
            "🔄 Attempting to install/upgrade non-ML core dependencies to user site...\n",
            "📦 Processing matplotlib>=3.5.0...\n",
            "   Executing: /usr/bin/python3 -m pip install --no-cache-dir --upgrade --user matplotlib>=3.5.0\n",
            "✅ Successfully processed matplotlib>=3.5.0.\n",
            "📦 Processing scipy>=1.12.0...\n",
            "   Executing: /usr/bin/python3 -m pip install --no-cache-dir --upgrade --user scipy>=1.12.0\n",
            "✅ Successfully processed scipy>=1.12.0.\n",
            "📦 Processing protobuf>=3.19.0,<4.25.0...\n",
            "   Executing: /usr/bin/python3 -m pip install --no-cache-dir --upgrade --user protobuf>=3.19.0,<4.25.0\n",
            "✅ Successfully processed protobuf>=3.19.0,<4.25.0.\n",
            "📦 Processing pandas>=1.3.0...\n",
            "   Executing: /usr/bin/python3 -m pip install --no-cache-dir --upgrade --user pandas>=1.3.0\n",
            "✅ Successfully processed pandas>=1.3.0.\n",
            "📦 Processing scikit-learn>=1.0.0...\n",
            "   Executing: /usr/bin/python3 -m pip install --no-cache-dir --upgrade --user scikit-learn>=1.0.0\n",
            "✅ Successfully processed scikit-learn>=1.0.0.\n",
            "✅ Non-ML core dependency processing complete.\n",
            "Adding /root/.local/lib/python3.11/site-packages to sys.path (priority 0)\n",
            "🔄 Importing Colab's default NumPy (post any other pip installs)...\n",
            "✅ NumPy version loaded: 2.0.2 from /usr/local/lib/python3.11/dist-packages/numpy/__init__.py\n",
            "🔄 Importing Colab's default TensorFlow (post any other pip installs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ TensorFlow version loaded: 2.18.0 from /usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\n",
            "🔧 Setting up TensorFlow compatibility...\n",
            "📍 Using TensorFlow version: 2.18.0\n",
            "📍 Using NumPy version: 2.0.2\n",
            "ℹ️ NumPy version is 2.0.2. Colab's TensorFlow (2.18.0) should be compatible (e.g., >=2.16 or specially built).\n",
            "📍 ml_dtypes version found: 0.4.1 (from /usr/local/lib/python3.11/dist-packages/ml_dtypes/__init__.py)\n",
            "✅ TensorFlow (Colab's default) configured for v1 compatibility mode.\n",
            "⚠️ No GPU detected, will use CPU.\n",
            "🔍 Verifying Google Colab Environment (using Colab defaults for TF/NumPy)...\n",
            "📍 Python: 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n",
            "📍 sys.path (first few entries): ['/root/.local/lib/python3.11/site-packages', '/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11']\n",
            "📍 Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "📍 Architecture: x86_64\n",
            "📍 NumPy Version (loaded): 2.0.2\n",
            "📍 NumPy Path: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py\n",
            "📍 TensorFlow Version (loaded): 2.18.0\n",
            "📍 TensorFlow Path: /usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\n",
            "📍 ml_dtypes Version (loaded): 0.4.1 from /usr/local/lib/python3.11/dist-packages/ml_dtypes/__init__.py\n",
            "📍 tensorflow-metadata: Not found by script or error (may not be needed or part of default TF).\n",
            "📍 Available RAM: 12.7 GB\n",
            "📍 Available disk space: 70.3 GB\n",
            "✅ Environment setup attempt complete using Colab's default TF/NumPy (or best effort)!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================\n",
        "# CELL 1: Environment Setup - PRIORITIZING COLAB DEFAULTS for TF/NumPy\n",
        "# ========================================================================\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import site\n",
        "\n",
        "def install_missing_dependencies():\n",
        "    \"\"\"Install/upgrade non-ML core dependencies. NumPy and TensorFlow should be Colab's defaults.\"\"\"\n",
        "    print(\"🔄 Installing/upgrading non-ML core dependencies for HardGNN...\")\n",
        "    print(f\"📍 Detected Python version: {sys.version}\")\n",
        "\n",
        "    # We will NOT install numpy, tensorflow, or ml-dtypes via pip.\n",
        "    # We rely on Colab's pre-installed versions.\n",
        "    dependencies = [\n",
        "        \"matplotlib>=3.5.0\",\n",
        "        \"scipy>=1.12.0\",\n",
        "        \"protobuf>=3.19.0,<4.25.0\",\n",
        "        \"pandas>=1.3.0\",\n",
        "        \"scikit-learn>=1.0.0\"\n",
        "    ]\n",
        "\n",
        "    print(\"🔄 Attempting to install/upgrade non-ML core dependencies to user site...\")\n",
        "    for dep in dependencies:\n",
        "        print(f\"📦 Processing {dep}...\")\n",
        "        try:\n",
        "            # Using --upgrade will install if not present, or upgrade if it is.\n",
        "            # --user to keep it in user space.\n",
        "            command = [sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"--upgrade\", \"--user\", dep]\n",
        "            print(f\"   Executing: {' '.join(command)}\")\n",
        "            result = subprocess.run(command,\n",
        "                                  check=True, capture_output=True, text=True, timeout=180)\n",
        "            print(f\"✅ Successfully processed {dep}.\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠️ Warning: Could not process {dep}. Pip stdout: {e.stdout.strip()}. Pip stderr: {e.stderr.strip()}\")\n",
        "        except subprocess.TimeoutExpired as e:\n",
        "            print(f\"⚠️ Timeout: Processing of {dep} took too long. Pip stdout: {e.stdout.strip()}. Pip stderr: {e.stderr.strip()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ An unexpected error occurred processing {dep}: {e}\")\n",
        "\n",
        "    print(\"✅ Non-ML core dependency processing complete.\")\n",
        "    try:\n",
        "        # Ensure user site packages are in path\n",
        "        if hasattr(site, 'USER_SITE') and site.USER_SITE and site.USER_SITE not in sys.path:\n",
        "            print(f\"Adding {site.USER_SITE} to sys.path (priority 0)\")\n",
        "            sys.path.insert(0, site.USER_SITE)\n",
        "        # For Colab/Linux, also consider adding local/bin to PATH if it exists for any pip installed CLIs\n",
        "        local_bin_path = os.path.expanduser(\"~/.local/bin\")\n",
        "        if os.path.isdir(local_bin_path) and local_bin_path not in os.environ['PATH']:\n",
        "            print(f\"Adding {local_bin_path} to PATH\")\n",
        "            os.environ['PATH'] = local_bin_path + os.pathsep + os.environ['PATH']\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not robustly update sys.path/PATH for user site: {e}\")\n",
        "\n",
        "def setup_tensorflow_compatibility(tf_module, numpy_module):\n",
        "    print(f\"🔧 Setting up TensorFlow compatibility...\")\n",
        "    print(f\"📍 Using TensorFlow version: {tf_module.__version__ if tf_module else 'N/A'}\")\n",
        "    print(f\"📍 Using NumPy version: {numpy_module.__version__ if numpy_module else 'N/A'}\")\n",
        "\n",
        "    if not tf_module or not numpy_module:\n",
        "        print(\"❌ CRITICAL: TensorFlow or NumPy module not available. Cannot proceed with setup.\")\n",
        "        return False\n",
        "\n",
        "    # Informational checks about loaded versions\n",
        "    if numpy_module.__version__.startswith(\"2.\"):\n",
        "        print(f\"ℹ️ NumPy version is {numpy_module.__version__}. Colab's TensorFlow ({tf_module.__version__}) should be compatible (e.g., >=2.16 or specially built).\" )\n",
        "    elif numpy_module.__version__.startswith(\"1.\"):\n",
        "        print(f\"ℹ️ NumPy version is {numpy_module.__version__}. Colab's TensorFlow ({tf_module.__version__}) should be compatible (e.g., <=2.15 or specially built).\" )\n",
        "    else:\n",
        "        print(f\"⚠️ Unknown NumPy version pattern: {numpy_module.__version__}\")\n",
        "\n",
        "    try:\n",
        "        import ml_dtypes\n",
        "        print(f\"📍 ml_dtypes version found: {ml_dtypes.__version__} (from {ml_dtypes.__file__})\")\n",
        "        if numpy_module.__version__.startswith(\"2.\") and not ml_dtypes.__version__.startswith((\"0.4\", \"0.5\")):\n",
        "            print(f\"   ⚠️ WARNING: ml_dtypes version ({ml_dtypes.__version__}) might not be ideal for NumPy 2.x (expected 0.4.x or 0.5.x). Check for runtime issues.\")\n",
        "        elif numpy_module.__version__.startswith(\"1.\") and ml_dtypes.__version__.startswith((\"0.4\", \"0.5\")):\n",
        "             print(f\"   ⚠️ WARNING: ml_dtypes version ({ml_dtypes.__version__}) might not be ideal for NumPy 1.x (expected <0.4.x). Check for runtime issues.\")\n",
        "    except ImportError:\n",
        "        print(\"ℹ️ ml_dtypes not explicitly found or importable by script. TensorFlow might bundle it or manage it internally.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error during ml_dtypes check: {e}\")\n",
        "\n",
        "    try:\n",
        "        tf_module.compat.v1.disable_eager_execution()\n",
        "        tf_module.compat.v1.disable_v2_behavior()\n",
        "        print(\"✅ TensorFlow (Colab's default) configured for v1 compatibility mode.\")\n",
        "\n",
        "        gpus = tf_module.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            print(f\"🚀 GPU acceleration available: {len(gpus)} GPU(s) detected\")\n",
        "            for gpu_device in gpus:\n",
        "                print(f\"   - {gpu_device}\")\n",
        "                try:\n",
        "                    tf_module.config.experimental.set_memory_growth(gpu_device, True)\n",
        "                    print(f\"✅ GPU memory growth configured for {gpu_device}\")\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"⚠️ Could not configure GPU memory growth for {gpu_device}: {e}\")\n",
        "        else:\n",
        "            print(\"⚠️ No GPU detected, will use CPU.\")\n",
        "        return True\n",
        "    except AttributeError as e:\n",
        "        print(f\"❌ AttributeError during TensorFlow v1 compatibility setup: {e}\")\n",
        "        print(f\"   This can happen if Colab's TensorFlow version ({tf_module.__version__}) is too old, or has an unexpected structure, or is incompatible with its NumPy ({numpy_module.__version__}).\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error setting up TensorFlow v1 compatibility layer: {e}\")\n",
        "        return False\n",
        "\n",
        "def verify_colab_environment(tf_module, numpy_module):\n",
        "    import sys\n",
        "    import psutil\n",
        "    import platform\n",
        "\n",
        "    print(\"🔍 Verifying Google Colab Environment (using Colab defaults for TF/NumPy)...\")\n",
        "    print(f\"📍 Python: {sys.version}\")\n",
        "    print(f\"📍 sys.path (first few entries): {str(sys.path[:5])}\")\n",
        "    print(f\"📍 Platform: {platform.platform()}\")\n",
        "    print(f\"📍 Architecture: {platform.machine()}\")\n",
        "\n",
        "    if numpy_module:\n",
        "        print(f\"📍 NumPy Version (loaded): {numpy_module.__version__}\")\n",
        "        print(f\"📍 NumPy Path: {numpy_module.__file__}\")\n",
        "    else:\n",
        "        print(\"📍 NumPy Version (loaded): NOT LOADED\")\n",
        "\n",
        "    if tf_module:\n",
        "        print(f\"📍 TensorFlow Version (loaded): {tf_module.__version__}\")\n",
        "        print(f\"📍 TensorFlow Path: {tf_module.__file__}\")\n",
        "    else:\n",
        "        print(\"📍 TensorFlow Version (loaded): NOT LOADED\")\n",
        "\n",
        "    try:\n",
        "        import ml_dtypes\n",
        "        print(f\"📍 ml_dtypes Version (loaded): {ml_dtypes.__version__} from {ml_dtypes.__file__}\")\n",
        "    except Exception:\n",
        "        print(f\"📍 ml_dtypes: Not found by script or error during import check (may be internal to TF).\")\n",
        "\n",
        "    try:\n",
        "        import tensorflow_metadata # Check if it's part of Colab's default TF environment\n",
        "        print(f\"📍 tensorflow-metadata Version (loaded): {tensorflow_metadata.__version__} from {tensorflow_metadata.__file__}\")\n",
        "    except Exception:\n",
        "        print(f\"📍 tensorflow-metadata: Not found by script or error (may not be needed or part of default TF).\")\n",
        "\n",
        "    memory = psutil.virtual_memory()\n",
        "    memory_gb = memory.total / (1024**3)\n",
        "    print(f\"📍 Available RAM: {memory_gb:.1f} GB\")\n",
        "    disk = psutil.disk_usage('/')\n",
        "    disk_gb = disk.free / (1024**3)\n",
        "    print(f\"📍 Available disk space: {disk_gb:.1f} GB\")\n",
        "    return True\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "print(\"=\" * 60)\n",
        "print(\"🚀 HardGNN Setup for Google Colab Pro+ (PRIORITIZING COLAB DEFAULTS for TF/NumPy)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Install/Upgrade other dependencies (non TF/NumPy)\n",
        "install_missing_dependencies()\n",
        "\n",
        "# 2. Import Colab's default NumPy and TensorFlow\n",
        "# These imports will now occur *after* pip has potentially modified the environment\n",
        "# by installing other packages and their dependencies, and after sys.path modifications.\n",
        "print(\"🔄 Importing Colab's default NumPy (post any other pip installs)...\")\n",
        "numpy_to_use = None\n",
        "tensorflow_to_use = None\n",
        "\n",
        "try:\n",
        "    import numpy\n",
        "    numpy_to_use = numpy\n",
        "    print(f\"✅ NumPy version loaded: {numpy_to_use.__version__} from {numpy_to_use.__file__}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ FAILED to import Colab's default NumPy: {e}\")\n",
        "    print(\"   This is a critical failure. Further steps will likely fail.\")\n",
        "\n",
        "print(\"🔄 Importing Colab's default TensorFlow (post any other pip installs)...\")\n",
        "try:\n",
        "    import tensorflow\n",
        "    tensorflow_to_use = tensorflow\n",
        "    print(f\"✅ TensorFlow version loaded: {tensorflow_to_use.__version__} from {tensorflow_to_use.__file__}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ FAILED to import Colab's default TensorFlow: {e}\")\n",
        "    print(f\"   This could be due to an underlying issue with its dependencies (like the loaded NumPy version) or Colab environment configuration.\")\n",
        "\n",
        "# Check if imports were successful before proceeding\n",
        "if not numpy_to_use or not tensorflow_to_use:\n",
        "    # Allow script to continue to verify_colab_environment to see more details if one failed\n",
        "    print(\"⚠️ CRITICAL FAILURE: Could not import Colab's default NumPy or TensorFlow. Environment setup will likely be incomplete or fail.\")\n",
        "    # We will let it proceed to verify_colab_environment and then the final check for setup_successful\n",
        "    # rather than raising an immediate RuntimeError here, to get more diagnostic output.\n",
        "\n",
        "# 3. Setup TensorFlow compatibility using the imported Colab modules\n",
        "setup_successful = False # Default to False\n",
        "if numpy_to_use and tensorflow_to_use:\n",
        "    setup_successful = setup_tensorflow_compatibility(tf_module=tensorflow_to_use, numpy_module=numpy_to_use)\n",
        "else:\n",
        "    print(\"Skipping TensorFlow compatibility setup as core modules (NumPy/TensorFlow) failed to load.\")\n",
        "\n",
        "# 4. Verify environment using the imported Colab modules\n",
        "verify_colab_environment(tf_module=tensorflow_to_use, numpy_module=numpy_to_use)\n",
        "\n",
        "if not setup_successful:\n",
        "    # Custom error message based on whether TF/NumPy even loaded\n",
        "    if not numpy_to_use or not tensorflow_to_use:\n",
        "        raise RuntimeError(\"❌ TensorFlow/NumPy native import failed. Cannot configure environment.\")\n",
        "    else:\n",
        "        raise RuntimeError(\"❌ TensorFlow setup failed using Colab's default versions. There might be an incompatibility within the pre-built Colab environment, or the TF1 compatibility layer cannot be applied to the loaded versions.\")\n",
        "\n",
        "print(\"✅ Environment setup attempt complete using Colab's default TF/NumPy (or best effort)!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSFLoss_oiYy",
        "outputId": "233af8ca-e210-44ee-f934-bc2273a4e44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Importing HardGNN model...\n",
            "✅ Using TensorFlow 2.x compatible HardGNN model (PRODUCTION)\n",
            "TensorFlow version: 2.18.0\n",
            "✅ Successfully imported HardGNN model\n",
            "✅ HardGNN modules imported and configured successfully\n",
            "📊 Configuration for GOWALLA Dataset:\n",
            "  Dataset: gowalla\n",
            "  Learning Rate: 0.002\n",
            "  Regularization: 0.01\n",
            "  Temperature (τ): 0.1\n",
            "  SSL Regularization: 1e-06\n",
            "  Batch Size: 512\n",
            "  Graph Number: 3\n",
            "  GNN Layers: 2\n",
            "  Attention Layers: 1\n",
            "🔥 Hard Negative Sampling Configuration:\n",
            "  Enabled: True\n",
            "  Hard Negatives (K): 5\n",
            "  Contrastive Weight (λ): 0.1\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================\n",
        "# CELL 2: Dataset Configuration and Module Import\n",
        "# ========================================================================\n",
        "# Make the globally configured TensorFlow available as tf\n",
        "if tensorflow_to_use: # Check if tensorflow_to_use was successfully imported in Cell 1\n",
        "    tf = tensorflow_to_use\n",
        "else:\n",
        "    # Fallback or error if tensorflow_to_use didn't load, though Cell 1 should raise an error earlier.\n",
        "    # This import might fail if Cell 1 failed catastrophically before setting tensorflow_to_use.\n",
        "    import tensorflow as tf\n",
        "    print(\"⚠️ Warning: tensorflow_to_use was not set from Cell 1. Attempted direct import of tensorflow as tf.\")\n",
        "\n",
        "# Core imports\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from ast import arg\n",
        "from random import randint\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Import our modules\n",
        "from Params import args\n",
        "import Utils.TimeLogger as logger\n",
        "from Utils.TimeLogger import log\n",
        "from DataHandler import DataHandler\n",
        "\n",
        "# Import the HardGNN model\n",
        "print(\"\\n🔧 Importing HardGNN model...\")\n",
        "try:\n",
        "    from HardGNN_model import Recommender\n",
        "    print(\"✅ Successfully imported HardGNN model\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Failed to import HardGNN model: {e}\")\n",
        "    print(\"Please ensure all dependencies are properly installed.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "def configure_dataset(dataset_name):\n",
        "    \"\"\"Configure parameters based on validated configurations for each dataset\"\"\"\n",
        "\n",
        "    # Set base dataset\n",
        "    args.data = dataset_name.lower()\n",
        "\n",
        "    # Dataset-specific validated configurations\n",
        "    if dataset_name.lower() == 'yelp':\n",
        "        # From yelp.sh - validated configuration\n",
        "        args.lr = 1e-3\n",
        "        args.reg = 1e-2\n",
        "        args.temp = 0.1\n",
        "        args.ssl_reg = 1e-7\n",
        "        args.epoch = 150\n",
        "        args.batch = 512\n",
        "        args.sslNum = 40\n",
        "        args.graphNum = 12\n",
        "        args.gnn_layer = 3\n",
        "        args.att_layer = 2\n",
        "        args.testSize = 1000\n",
        "        args.ssldim = 32\n",
        "        args.sampNum = 40\n",
        "\n",
        "    elif dataset_name.lower() == 'amazon':\n",
        "        # From amazon.sh - validated configuration\n",
        "        args.lr = 1e-3\n",
        "        args.reg = 1e-2\n",
        "        args.temp = 0.1\n",
        "        args.ssl_reg = 1e-6\n",
        "        args.epoch = 150\n",
        "        args.batch = 512\n",
        "        args.sslNum = 80\n",
        "        args.graphNum = 5\n",
        "        args.pred_num = 0\n",
        "        args.gnn_layer = 3\n",
        "        args.att_layer = 4\n",
        "        args.testSize = 1000\n",
        "        args.keepRate = 0.5\n",
        "        args.sampNum = 40\n",
        "        args.pos_length = 200\n",
        "\n",
        "    elif dataset_name.lower() == 'gowalla':\n",
        "        # From gowalla.sh - validated configuration\n",
        "        args.lr = 2e-3\n",
        "        args.reg = 1e-2\n",
        "        args.temp = 0.1\n",
        "        args.ssl_reg = 1e-6\n",
        "        args.epoch = 150\n",
        "        args.batch = 512\n",
        "        args.graphNum = 3\n",
        "        args.gnn_layer = 2\n",
        "        args.att_layer = 1\n",
        "        args.testSize = 1000\n",
        "        args.sampNum = 40\n",
        "\n",
        "    elif dataset_name.lower() == 'movielens':\n",
        "        # From movielens.sh - validated configuration\n",
        "        args.lr = 1e-3\n",
        "        args.reg = 1e-2\n",
        "        args.ssl_reg = 1e-6\n",
        "        args.epoch = 150\n",
        "        args.batch = 512\n",
        "        args.sampNum = 40\n",
        "        args.sslNum = 90\n",
        "        args.graphNum = 6\n",
        "        args.gnn_layer = 2\n",
        "        args.att_layer = 3\n",
        "        args.testSize = 1000\n",
        "        args.ssldim = 48\n",
        "        args.keepRate = 0.5\n",
        "        args.pos_length = 200\n",
        "        args.leaky = 0.5\n",
        "\n",
        "    else:\n",
        "        print(f\"⚠️  Unknown dataset: {dataset_name}\")\n",
        "        print(\"Available datasets: yelp, amazon, gowalla, movielens\")\n",
        "        print(\"Using default parameters...\")\n",
        "\n",
        "    # Add hard negative sampling parameters (consistent across all datasets)\n",
        "    args.use_hard_neg = True\n",
        "    args.hard_neg_top_k = 5      # K = 5 hard negatives\n",
        "    args.contrastive_weight = 0.1 # λ = 0.1 contrastive weight\n",
        "    # Note: τ (temperature) is already set in args.temp = 0.1\n",
        "\n",
        "    # Full experiment settings (demo mode removed)\n",
        "    # args.epoch is now set by the dataset-specific configurations in the if/elif blocks above\n",
        "    # args.trnNum is now set by dataset-specific configurations or defaults\n",
        "    # To ensure full training data is used, we'll rely on the dataset handler's defaults\n",
        "    # for trnNum unless explicitly set by a dataset config block.\n",
        "    # If a dataset config block (e.g., for 'amazon') sets args.trnNum, that will be used.\n",
        "    # Otherwise, the DataHandler will likely use all available training users.\n",
        "\n",
        "    args.tstEpoch = 3  # Test every 3 epochs (can be adjusted if needed for full runs)\n",
        "\n",
        "    # Set save path\n",
        "    args.save_path = f'hardgnn_{dataset_name.lower()}_colab'\n",
        "\n",
        "    return args\n",
        "\n",
        "# Configure the dataset\n",
        "configure_dataset(DATASET)\n",
        "\n",
        "print(\"✅ HardGNN modules imported and configured successfully\")\n",
        "print(f\"📊 Configuration for {DATASET.upper()} Dataset:\")\n",
        "print(f\"  Dataset: {args.data}\")\n",
        "print(f\"  Learning Rate: {args.lr}\")\n",
        "print(f\"  Regularization: {args.reg}\")\n",
        "print(f\"  Temperature (τ): {args.temp}\")\n",
        "print(f\"  SSL Regularization: {args.ssl_reg}\")\n",
        "print(f\"  Batch Size: {args.batch}\")\n",
        "print(f\"  Graph Number: {args.graphNum}\")\n",
        "print(f\"  GNN Layers: {args.gnn_layer}\")\n",
        "print(f\"  Attention Layers: {args.att_layer}\")\n",
        "print(\"🔥 Hard Negative Sampling Configuration:\")\n",
        "print(f\"  Enabled: {args.use_hard_neg}\")\n",
        "print(f\"  Hard Negatives (K): {args.hard_neg_top_k}\")\n",
        "print(f\"  Contrastive Weight (λ): {args.contrastive_weight}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqYwGgj0_8CH",
        "outputId": "8deebda9-b143-4b0f-9562-ac22a9ad838f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-29 05:29:59.599535: 🔄 Starting gowalla data loading...\n",
            "tstInt [None None None ... None None None]\n",
            "tstStat [False False False ... False False False] 48653\n",
            "tstUsrs [    7     8    21 ... 48625 48627 48636] 10000\n",
            "trnMat <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 1141825 stored elements and shape (48653, 52621)>\n",
            "  Coords\tValues\n",
            "  (0, 1)\t28.0\n",
            "  (0, 2)\t2.0\n",
            "  (0, 3)\t1.0\n",
            "  (0, 4)\t1.0\n",
            "  (0, 5)\t1.0\n",
            "  (0, 6)\t14.0\n",
            "  (0, 7)\t2.0\n",
            "  (0, 8)\t2.0\n",
            "  (0, 9)\t1.0\n",
            "  (0, 10)\t1.0\n",
            "  (0, 11)\t1.0\n",
            "  (0, 12)\t1.0\n",
            "  (0, 13)\t1.0\n",
            "  (0, 14)\t1.0\n",
            "  (0, 15)\t2.0\n",
            "  (0, 16)\t1.0\n",
            "  (0, 17)\t1.0\n",
            "  (0, 18)\t1.0\n",
            "  (0, 19)\t1.0\n",
            "  (0, 20)\t1.0\n",
            "  (0, 21)\t1.0\n",
            "  (0, 22)\t1.0\n",
            "  (0, 23)\t1.0\n",
            "  (0, 24)\t1.0\n",
            "  (0, 25)\t1.0\n",
            "  :\t:\n",
            "  (48649, 46304)\t1.0\n",
            "  (48650, 20437)\t1.0\n",
            "  (48650, 26212)\t1.0\n",
            "  (48650, 31914)\t1.0\n",
            "  (48650, 39530)\t1.0\n",
            "  (48650, 39533)\t1.0\n",
            "  (48650, 41640)\t1.0\n",
            "  (48650, 41641)\t1.0\n",
            "  (48650, 41642)\t2.0\n",
            "  (48650, 41643)\t1.0\n",
            "  (48650, 41644)\t1.0\n",
            "  (48650, 41645)\t1.0\n",
            "  (48650, 41646)\t1.0\n",
            "  (48650, 41647)\t1.0\n",
            "  (48650, 45497)\t1.0\n",
            "  (48650, 51185)\t1.0\n",
            "  (48650, 51665)\t1.0\n",
            "  (48651, 18917)\t1.0\n",
            "  (48651, 25646)\t1.0\n",
            "  (48651, 47862)\t2.0\n",
            "  (48651, 49299)\t1.0\n",
            "  (48652, 18415)\t1.0\n",
            "  (48652, 18537)\t16.0\n",
            "  (48652, 18544)\t1.0\n",
            "  (48652, 26915)\t1.0 [<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 299532 stored elements and shape (48653, 52621)>, <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 392642 stored elements and shape (48653, 52621)>, <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 545590 stored elements and shape (48653, 52621)>] <Compressed Sparse Row sparse matrix of dtype 'int32'\n",
            "\twith 842293 stored elements and shape (48653, 52621)>\n",
            "  Coords\tValues\n",
            "  (0, 1)\t1\n",
            "  (0, 2)\t2\n",
            "  (0, 3)\t2\n",
            "  (0, 4)\t2\n",
            "  (0, 5)\t2\n",
            "  (0, 6)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 8)\t2\n",
            "  (0, 9)\t2\n",
            "  (0, 10)\t2\n",
            "  (0, 11)\t2\n",
            "  (0, 12)\t2\n",
            "  (0, 13)\t2\n",
            "  (0, 14)\t2\n",
            "  (0, 15)\t2\n",
            "  (0, 16)\t2\n",
            "  (0, 17)\t2\n",
            "  (0, 18)\t2\n",
            "  (0, 19)\t2\n",
            "  (0, 20)\t2\n",
            "  (0, 21)\t2\n",
            "  (0, 22)\t2\n",
            "  (0, 23)\t2\n",
            "  (0, 24)\t2\n",
            "  (0, 25)\t2\n",
            "  :\t:\n",
            "  (48649, 46304)\t2\n",
            "  (48650, 26212)\t2\n",
            "  (48650, 51185)\t2\n",
            "  (48650, 31914)\t2\n",
            "  (48650, 20437)\t2\n",
            "  (48650, 39530)\t2\n",
            "  (48650, 39533)\t2\n",
            "  (48650, 51665)\t2\n",
            "  (48650, 41646)\t2\n",
            "  (48650, 45497)\t2\n",
            "  (48650, 41647)\t2\n",
            "  (48650, 41641)\t2\n",
            "  (48650, 41643)\t2\n",
            "  (48650, 41642)\t2\n",
            "  (48650, 41640)\t2\n",
            "  (48650, 41644)\t2\n",
            "  (48650, 41645)\t2\n",
            "  (48651, 49299)\t2\n",
            "  (48651, 47862)\t2\n",
            "  (48651, 18917)\t2\n",
            "  (48651, 25646)\t2\n",
            "  (48652, 18537)\t2\n",
            "  (48652, 18544)\t2\n",
            "  (48652, 18415)\t2\n",
            "  (48652, 26915)\t2\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 299532 stored elements and shape (48653, 52621)>\n",
            "  Coords\tValues\n",
            "  (3, 81)\t1267476265.0\n",
            "  (3, 489)\t1268568521.0\n",
            "  (3, 490)\t13943569735.0\n",
            "  (3, 491)\t1267982719.0\n",
            "  (3, 492)\t1267872253.0\n",
            "  (3, 493)\t2535146132.0\n",
            "  (3, 494)\t1267611878.0\n",
            "  (3, 495)\t2534387268.0\n",
            "  (3, 496)\t2534291289.0\n",
            "  (3, 497)\t1267378306.0\n",
            "  (3, 498)\t1267175910.0\n",
            "  (3, 499)\t1267118877.0\n",
            "  (3, 500)\t1267092817.0\n",
            "  (3, 501)\t1267007333.0\n",
            "  (3, 502)\t2533647663.0\n",
            "  (3, 503)\t1266925388.0\n",
            "  (3, 504)\t1266680270.0\n",
            "  (3, 505)\t1266663990.0\n",
            "  (3, 506)\t1266615112.0\n",
            "  (3, 507)\t1266599758.0\n",
            "  (3, 508)\t1266401259.0\n",
            "  (7, 554)\t2535347715.0\n",
            "  (7, 555)\t1265911829.0\n",
            "  (7, 564)\t2530931085.0\n",
            "  (7, 579)\t1270229529.0\n",
            "  :\t:\n",
            "  (48612, 6943)\t3809089267.0\n",
            "  (48612, 6957)\t1270290002.0\n",
            "  (48612, 6972)\t1270282709.0\n",
            "  (48612, 6990)\t2538791063.0\n",
            "  (48612, 6992)\t2539377596.0\n",
            "  (48612, 6993)\t2538778818.0\n",
            "  (48612, 7820)\t1270304456.0\n",
            "  (48612, 12640)\t1270284853.0\n",
            "  (48612, 18857)\t1269188788.0\n",
            "  (48612, 31027)\t1270223266.0\n",
            "  (48614, 904)\t1270446656.0\n",
            "  (48614, 21177)\t1270440024.0\n",
            "  (48614, 36055)\t1268492199.0\n",
            "  (48621, 26593)\t1270020620.0\n",
            "  (48621, 43444)\t1270020650.0\n",
            "  (48621, 47262)\t1270020369.0\n",
            "  (48637, 3157)\t1268561625.0\n",
            "  (48637, 8923)\t1267901709.0\n",
            "  (48637, 15035)\t1266176022.0\n",
            "  (48637, 15117)\t1268422195.0\n",
            "  (48637, 28088)\t1266018543.0\n",
            "  (48645, 16122)\t1270622425.0\n",
            "  (48645, 17869)\t1270705656.0\n",
            "  (48645, 17910)\t1270704750.0\n",
            "  (48645, 17935)\t1270623573.0 <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 545590 stored elements and shape (48653, 52621)>\n",
            "  Coords\tValues\n",
            "  (0, 1)\t25678447512.0\n",
            "  (0, 2)\t2566721081.0\n",
            "  (0, 3)\t1287314765.0\n",
            "  (0, 4)\t1287226242.0\n",
            "  (0, 5)\t1286899083.0\n",
            "  (0, 6)\t12841649742.0\n",
            "  (0, 7)\t1286870240.0\n",
            "  (0, 8)\t2573250548.0\n",
            "  (0, 9)\t1286799680.0\n",
            "  (0, 10)\t1286799642.0\n",
            "  (0, 11)\t1286719237.0\n",
            "  (0, 12)\t1286716634.0\n",
            "  (0, 13)\t1286704024.0\n",
            "  (0, 14)\t1286634427.0\n",
            "  (0, 15)\t2573262931.0\n",
            "  (0, 16)\t1286629523.0\n",
            "  (0, 17)\t1286559423.0\n",
            "  (0, 18)\t1286545717.0\n",
            "  (0, 19)\t1286545548.0\n",
            "  (0, 20)\t1286545432.0\n",
            "  (0, 21)\t1286545382.0\n",
            "  (0, 22)\t1286529867.0\n",
            "  (0, 23)\t1286467668.0\n",
            "  (0, 24)\t1286464690.0\n",
            "  (0, 25)\t1286459911.0\n",
            "  :\t:\n",
            "  (48649, 46304)\t1284944041.0\n",
            "  (48650, 20437)\t1287305045.0\n",
            "  (48650, 26212)\t1287316251.0\n",
            "  (48650, 31914)\t1287312070.0\n",
            "  (48650, 39530)\t1287303882.0\n",
            "  (48650, 39533)\t1287303818.0\n",
            "  (48650, 41640)\t1287283032.0\n",
            "  (48650, 41641)\t1287287810.0\n",
            "  (48650, 41642)\t2574501027.0\n",
            "  (48650, 41643)\t1287285848.0\n",
            "  (48650, 41644)\t1287282995.0\n",
            "  (48650, 41645)\t1287282761.0\n",
            "  (48650, 41646)\t1287292554.0\n",
            "  (48650, 41647)\t1287291219.0\n",
            "  (48650, 45497)\t1287292529.0\n",
            "  (48650, 51185)\t1287314811.0\n",
            "  (48650, 51665)\t1287303485.0\n",
            "  (48651, 18917)\t1285785205.0\n",
            "  (48651, 25646)\t1285737003.0\n",
            "  (48651, 47862)\t2572897027.0\n",
            "  (48651, 49299)\t1286857183.0\n",
            "  (48652, 18415)\t1286048124.0\n",
            "  (48652, 18537)\t20572881560.0\n",
            "  (48652, 18544)\t1286645754.0\n",
            "  (48652, 26915)\t1284763478.0\n",
            "2025-05-29 05:30:12.466075: ✅ gowalla data loaded successfully\n",
            "📈 GOWALLA Dataset Statistics:\n",
            "  Users: 48,653\n",
            "  Items: 52,621\n",
            "  Training interactions: 1,148,589\n",
            "  Test users: 10,000\n",
            "  Time-based graphs: 3\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================\n",
        "# CELL 3: Load Dataset\n",
        "# ========================================================================\n",
        "\n",
        "# Initialize and load data\n",
        "logger.saveDefault = True\n",
        "log(f'🔄 Starting {DATASET} data loading...')\n",
        "\n",
        "handler = DataHandler()\n",
        "handler.LoadData()\n",
        "\n",
        "log(f'✅ {DATASET} data loaded successfully')\n",
        "print(f\"📈 {DATASET.upper()} Dataset Statistics:\")\n",
        "print(f\"  Users: {args.user:,}\")\n",
        "print(f\"  Items: {args.item:,}\")\n",
        "print(f\"  Training interactions: {handler.trnMat.nnz:,}\")\n",
        "print(f\"  Test users: {len(handler.tstUsrs):,}\")\n",
        "print(f\"  Time-based graphs: {len(handler.subMat)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmvkHxGgHBtz",
        "outputId": "997d42ed-8c78-4cf1-ed8e-44f01d39992b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Validating Hard Negative Sampling on gowalla...\n",
            "📊 Testing with τ=0.1, K=5, λ=0.1\n",
            "USER 48653 ITEM 52621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Google-Colab/DataHandler.py:51: RuntimeWarning: invalid value encountered in cast\n",
            "  data = coomat.data.astype(np.int32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-29 05:30:36.182572: ✅ Model initialized (random weights)\n",
            "\n",
            "============================================================\n",
            "🎯 HARD NEGATIVE SAMPLING VALIDATION - GOWALLA\n",
            "============================================================\n",
            "📊 Metrics:\n",
            "  Contrastive Loss: 5.054583\n",
            "  Supervised Loss: 3.908865\n",
            "  Positive Predictions: 16.3672 ± 9.8766\n",
            "  Negative Predictions: 12.9260 ± 7.9002\n",
            "  Prediction Gap: 3.4413\n",
            "  ✅ Positive predictions > Negative predictions\n",
            "  ✅ Hard negative sampling working correctly\n",
            "\n",
            "✅ Validation Complete - Ready for GOWALLA Training!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================\n",
        "# CELL 4: Validate Contrastive Loss Component\n",
        "# ========================================================================\n",
        "\n",
        "print(f\"🔍 Validating Hard Negative Sampling on {DATASET}...\")\n",
        "print(f\"📊 Testing with τ={args.temp}, K={args.hard_neg_top_k}, λ={args.contrastive_weight}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.compat.v1.set_random_seed(42)\n",
        "\n",
        "# Initialize TensorFlow session with GPU config\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n",
        "\n",
        "with tf.compat.v1.Session(config=config) as sess:\n",
        "    # Initialize HardGNN model\n",
        "    model = Recommender(sess, handler)\n",
        "    model.prepareModel()\n",
        "\n",
        "    # Initialize variables\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    log('✅ Model initialized (random weights)')\n",
        "\n",
        "    # Test contrastive loss on a small batch\n",
        "    test_users = handler.tstUsrs[:32]  # Small batch for validation\n",
        "\n",
        "    try:\n",
        "        # Sample batch with hard negatives\n",
        "        uLocs, iLocs, sequence, mask, uLocs_seq = model.sampleTrainBatch(\n",
        "            test_users, handler.trnMat, handler.timeMat, train_sample_num=10\n",
        "        )\n",
        "\n",
        "        # Sample SSL batch\n",
        "        suLocs, siLocs, suLocs_seq = model.sampleSslBatch(test_users, handler.subMat, False)\n",
        "\n",
        "        # Prepare feed dict\n",
        "        feed_dict = {\n",
        "            model.uids: uLocs,\n",
        "            model.iids: iLocs,\n",
        "            model.sequence: sequence,\n",
        "            model.mask: mask,\n",
        "            model.is_train: False,\n",
        "            model.uLocs_seq: uLocs_seq,\n",
        "            model.keepRate: 1.0\n",
        "        }\n",
        "\n",
        "        for k in range(args.graphNum):\n",
        "            feed_dict[model.suids[k]] = suLocs[k]\n",
        "            feed_dict[model.siids[k]] = siLocs[k]\n",
        "            feed_dict[model.suLocs_seq[k]] = suLocs_seq[k]\n",
        "\n",
        "        # Run forward pass\n",
        "        if hasattr(model, 'contrastive_loss'):\n",
        "            results = sess.run([\n",
        "                model.contrastive_loss,\n",
        "                model.preLoss,\n",
        "                model.posPred,\n",
        "                model.negPred\n",
        "            ], feed_dict=feed_dict)\n",
        "\n",
        "            contrastive_loss, pre_loss, pos_pred, neg_pred = results\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(f\"🎯 HARD NEGATIVE SAMPLING VALIDATION - {DATASET.upper()}\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"📊 Metrics:\")\n",
        "            print(f\"  Contrastive Loss: {contrastive_loss:.6f}\")\n",
        "            print(f\"  Supervised Loss: {pre_loss:.6f}\")\n",
        "            print(f\"  Positive Predictions: {np.mean(pos_pred):.4f} ± {np.std(pos_pred):.4f}\")\n",
        "            print(f\"  Negative Predictions: {np.mean(neg_pred):.4f} ± {np.std(neg_pred):.4f}\")\n",
        "            print(f\"  Prediction Gap: {np.mean(pos_pred) - np.mean(neg_pred):.4f}\")\n",
        "\n",
        "            if np.mean(pos_pred) > np.mean(neg_pred):\n",
        "                print(\"  ✅ Positive predictions > Negative predictions\")\n",
        "            else:\n",
        "                print(\"  ⚠️  Negative predictions >= Positive predictions\")\n",
        "\n",
        "            if contrastive_loss > 0 and not np.isnan(contrastive_loss):\n",
        "                print(\"  ✅ Hard negative sampling working correctly\")\n",
        "            else:\n",
        "                print(\"  ⚠️  Issue with hard negative sampling\")\n",
        "\n",
        "            print(f\"\\n✅ Validation Complete - Ready for {DATASET.upper()} Training!\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        else:\n",
        "            print(\"❌ Hard negative sampling not available\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Validation error: {e}\")\n",
        "        print(\"Proceeding with training...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJcciweGHG6T",
        "outputId": "e3bcc76c-db4b-4493-99d0-2decd8815e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting HardGNN Training on GOWALLA...\n",
            "📊 Training Configuration:\n",
            "  Dataset: gowalla\n",
            "  Epochs: 150\n",
            "  Test Frequency: Every 3 epochs\n",
            "  Training Instances: 10000\n",
            "  Batch Size: 512\n",
            "  Learning Rate: 0.002\n",
            "  Regularization: 0.01\n",
            "[DEBUG CELL 5] NNLayers_tf2.params before reset: ['uEmbed', 'iEmbed', 'posEmbed', 'timeEmbed', 'defaultParamName1', 'defaultParamName2', 'defaultParamName3', 'defaultParamName4', 'defaultParamName5', 'defaultParamName6', 'defaultParamName7', 'defaultParamName8', 'defaultParamName9', 'defaultParamName10', 'defaultParamName11', 'defaultParamName12', 'meta2', 'meta2Bias', 'meta3', 'meta3Bias']\n",
            "[DEBUG CELL 5] NNLayers_tf2.params after reset: Empty or None\n",
            "USER 48653 ITEM 52621\n",
            "[DEBUG CELL 5] NNLayers_tf2.params just before model.prepareModel(): Empty or None\n",
            "2025-05-29 05:32:27.838482: ✅ Model prepared for training\n",
            "2025-05-29 05:32:28.953265: ✅ Variables initialized\n",
            "\n",
            "================================================================================\n",
            "🎯 TRAINING HARDGNN ON GOWALLA WITH HARD NEGATIVE SAMPLING\n",
            "================================================================================\n",
            "\n",
            "📚 Epoch 1/150\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================\n",
        "# CELL 5: Train HardGNN Model\n",
        "# ========================================================================\n",
        "\n",
        "print(f\"🚀 Starting HardGNN Training on {DATASET.upper()}...\")\n",
        "print(f\"📊 Training Configuration:\")\n",
        "print(f\"  Dataset: {args.data}\")\n",
        "print(f\"  Epochs: {args.epoch}\")\n",
        "print(f\"  Test Frequency: Every {args.tstEpoch} epochs\")\n",
        "print(f\"  Training Instances: {args.trnNum}\")\n",
        "print(f\"  Batch Size: {args.batch}\")\n",
        "print(f\"  Learning Rate: {args.lr}\")\n",
        "print(f\"  Regularization: {args.reg}\")\n",
        "\n",
        "# Start fresh session for training\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Also reset NNLayers_tf2 global parameter tracking\n",
        "from Utils import NNLayers_tf2 # Import the module\n",
        "print(f\"[DEBUG CELL 5] NNLayers_tf2.params before reset: {list(NNLayers_tf2.params.keys()) if NNLayers_tf2.params else 'Empty or None'}\")\n",
        "NNLayers_tf2.reset_nn_params() # Call the reset function via the module\n",
        "print(f\"[DEBUG CELL 5] NNLayers_tf2.params after reset: {list(NNLayers_tf2.params.keys()) if NNLayers_tf2.params else 'Empty or None'}\")\n",
        "\n",
        "\n",
        "with tf.compat.v1.Session(config=config) as sess:\n",
        "    # Initialize model\n",
        "    model = Recommender(sess, handler)\n",
        "    print(f\"[DEBUG CELL 5] NNLayers_tf2.params just before model.prepareModel(): {list(NNLayers_tf2.params.keys()) if NNLayers_tf2.params else 'Empty or None'}\")\n",
        "    model.prepareModel()\n",
        "    log('✅ Model prepared for training')\n",
        "\n",
        "    # Initialize variables\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    log('✅ Variables initialized')\n",
        "\n",
        "    # Training loop\n",
        "    max_ndcg = 0.0\n",
        "    max_res = dict()\n",
        "    max_epoch = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"🎯 TRAINING HARDGNN ON {DATASET.upper()} WITH HARD NEGATIVE SAMPLING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for ep in range(args.epoch):\n",
        "        # Training step\n",
        "        test = (ep % args.tstEpoch == 0)\n",
        "\n",
        "        print(f\"\\n📚 Epoch {ep+1}/{args.epoch}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_results = model.trainEpoch()\n",
        "\n",
        "        # Print training results\n",
        "        train_log = f\"🏋️  Train: Loss={train_results['Loss']:.4f}, PreLoss={train_results['preLoss']:.4f}\"\n",
        "        if 'contrastiveLoss' in train_results:\n",
        "            train_log += f\", ContrastiveLoss={train_results['contrastiveLoss']:.4f}\"\n",
        "        print(train_log)\n",
        "\n",
        "        # Test if it's a test epoch\n",
        "        if test:\n",
        "            test_results = model.testEpoch()\n",
        "            test_log = f\"🎯 Test: HR={test_results['HR']:.4f}, NDCG={test_results['NDCG']:.4f}\"\n",
        "            print(test_log)\n",
        "\n",
        "            # Track best results\n",
        "            if test_results['NDCG'] > max_ndcg:\n",
        "                max_ndcg = test_results['NDCG']\n",
        "                max_res = test_results.copy()\n",
        "                max_epoch = ep\n",
        "                print(f\"🌟 New best NDCG: {max_ndcg:.4f}\")\n",
        "\n",
        "    # Final test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"📊 FINAL RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    final_results = model.testEpoch()\n",
        "    print(f\"🎯 Final Test Results:\")\n",
        "    print(f\"  HR@10: {final_results['HR']:.4f}\")\n",
        "    print(f\"  NDCG@10: {final_results['NDCG']:.4f}\")\n",
        "\n",
        "    print(f\"\\n🏆 Best Results (Epoch {max_epoch}):\")\n",
        "    print(f\"  Best HR@10: {max_res.get('HR', 0):.4f}\")\n",
        "    print(f\"  Best NDCG@10: {max_res.get('NDCG', 0):.4f}\")\n",
        "\n",
        "    print(f\"\\n✅ HardGNN training on {DATASET.upper()} completed successfully!\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1GSm7ARHJK-"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# CELL 6: Optional - Compare with Baseline SelfGNN\n",
        "# ========================================================================\n",
        "\n",
        "# To compare with baseline, run this cell to train without hard negatives\n",
        "print(f\"🔬 Training Baseline SelfGNN on {DATASET.upper()} (without hard negatives) for comparison...\")\n",
        "\n",
        "# Disable hard negative sampling\n",
        "args.use_hard_neg = False\n",
        "print(f\"📊 Baseline Configuration: Hard Negative Sampling = {args.use_hard_neg}\")\n",
        "\n",
        "# Reset graph and train baseline\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "with tf.compat.v1.Session(config=config) as sess:\n",
        "    # Initialize baseline model\n",
        "    baseline_model = Recommender(sess, handler)\n",
        "    baseline_model.prepareModel()\n",
        "\n",
        "    # Initialize variables\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    log('✅ Baseline model initialized')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"📊 BASELINE SELFGNN TRAINING ON {DATASET.upper()}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    baseline_max_ndcg = 0.0\n",
        "    baseline_max_res = dict()\n",
        "\n",
        "    # Shorter training for comparison\n",
        "    for ep in range(min(15, args.epoch)):\n",
        "        test = (ep % args.tstEpoch == 0)\n",
        "\n",
        "        # Train\n",
        "        train_results = baseline_model.trainEpoch()\n",
        "        train_log = f\"Epoch {ep+1}: Loss={train_results['Loss']:.4f}, PreLoss={train_results['preLoss']:.4f}\"\n",
        "        print(train_log)\n",
        "\n",
        "        # Test\n",
        "        if test:\n",
        "            test_results = baseline_model.testEpoch()\n",
        "            test_log = f\"  Test: HR={test_results['HR']:.4f}, NDCG={test_results['NDCG']:.4f}\"\n",
        "            print(test_log)\n",
        "\n",
        "            if test_results['NDCG'] > baseline_max_ndcg:\n",
        "                baseline_max_ndcg = test_results['NDCG']\n",
        "                baseline_max_res = test_results.copy()\n",
        "\n",
        "    print(f\"\\n📊 Baseline Best Results:\")\n",
        "    print(f\"  HR@10: {baseline_max_res.get('HR', 0):.4f}\")\n",
        "    print(f\"  NDCG@10: {baseline_max_res.get('NDCG', 0):.4f}\")\n",
        "\n",
        "    print(f\"\\n🔍 Comparison Summary for {DATASET.upper()}:\")\n",
        "    improvement_hr = (max_res.get('HR', 0) - baseline_max_res.get('HR', 0)) / baseline_max_res.get('HR', 1) * 100\n",
        "    improvement_ndcg = (max_res.get('NDCG', 0) - baseline_max_res.get('NDCG', 0)) / baseline_max_res.get('NDCG', 1) * 100\n",
        "\n",
        "    print(f\"  HardGNN vs Baseline HR@10: {improvement_hr:+.2f}%\")\n",
        "    print(f\"  HardGNN vs Baseline NDCG@10: {improvement_ndcg:+.2f}%\")\n",
        "\n",
        "    if improvement_ndcg > 0:\n",
        "        print(\"  ✅ HardGNN shows improvement over baseline!\")\n",
        "    else:\n",
        "        print(\"  📝 Note: Longer training may be needed to see improvements\")\n",
        "\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD4PnYItHRD7"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# CELL 7: Results Analysis and Summary\n",
        "# ========================================================================\n",
        "\n",
        "print(f\"\"\"\n",
        "# 📈 Results Analysis - {DATASET.upper()} Dataset\n",
        "\n",
        "## Key Metrics to Monitor:\n",
        "\n",
        "1. **Contrastive Loss**: Should decrease over epochs, indicating better separation\n",
        "2. **HR@10**: Hit Ratio at 10 - higher is better\n",
        "3. **NDCG@10**: Normalized Discounted Cumulative Gain - higher is better\n",
        "4. **Prediction Gap**: Positive predictions should exceed negative predictions\n",
        "\n",
        "## HardGNN vs Baseline:\n",
        "- **Hard Negative Sampling** selects more challenging negatives using cosine similarity\n",
        "- **InfoNCE Loss** creates better decision boundaries with temperature scaling\n",
        "- **Integrated Training** balances supervised and contrastive objectives\n",
        "\n",
        "## 🎉 Summary\n",
        "\n",
        "You've successfully run **HardGNN** on the {DATASET.upper()} dataset!\n",
        "\n",
        "### What we accomplished:\n",
        "✅ **Used Validated Configuration**: Original proven hyperparameters for {DATASET}\n",
        "✅ **Hard Negative Sampling**: Cosine similarity-based selection of challenging negatives\n",
        "✅ **InfoNCE Contrastive Loss**: Temperature-scaled contrastive learning (τ=0.1)\n",
        "✅ **Integrated Training**: Balanced supervised + contrastive objectives (λ=0.1)\n",
        "✅ **GPU Acceleration**: Optimized for Colab Pro+ GPUs\n",
        "✅ **Dataset-Agnostic**: Works with any supported dataset\n",
        "\n",
        "### Key Takeaways:\n",
        "- **Validated Parameters**: Used proven configurations from original experiments\n",
        "- **Hard Negative Enhancement**: Added challenging negative sampling to improve learning\n",
        "- **Contrastive Learning**: InfoNCE loss helps create better decision boundaries\n",
        "- **Minimal Changes**: Only added hard negative sampling, kept everything else identical\n",
        "\n",
        "### Configuration Used for {DATASET.upper()}:\n",
        "- **Learning Rate**: {args.lr}\n",
        "- **Regularization**: {args.reg}\n",
        "- **Graph Number**: {args.graphNum}\n",
        "- **GNN Layers**: {args.gnn_layer}\n",
        "- **Attention Layers**: {args.att_layer}\n",
        "- **Temperature (τ)**: {args.temp}\n",
        "- **Hard Negatives (K)**: {args.hard_neg_top_k}\n",
        "- **Contrastive Weight (λ)**: {args.contrastive_weight}\n",
        "\n",
        "### Next Steps:\n",
        "- Try longer training (up to 150 epochs) for better convergence\n",
        "- Experiment with different K values (3, 5, 10) for hard negatives\n",
        "- Test different contrastive weights λ (0.05, 0.1, 0.2)\n",
        "- Compare with other datasets by changing DATASET parameter\n",
        "- Analyze attention patterns and embedding quality\n",
        "\n",
        "### To Run on Different Datasets:\n",
        "Change the DATASET parameter in Cell 1:\n",
        "```python\n",
        "DATASET = 'yelp'      # or 'amazon', 'gowalla', 'movielens'\n",
        "```\n",
        "\n",
        "**Citation**: This implementation extends the SelfGNN framework with hard negative sampling as described in Liu et al. (2024).\n",
        "\"\"\")\n",
        "\n",
        "# ========================================================================\n",
        "# END OF SCRIPT\n",
        "# ========================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-vZqIjFQzzB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}