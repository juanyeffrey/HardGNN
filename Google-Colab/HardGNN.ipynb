{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ScRbRSX_af9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# IMPORTANT: Update this path to where you uploaded the 'Google-Colab' folder!\n",
        "import os\n",
        "project_path = '/content/drive/MyDrive/Google-Colab'\n",
        "os.chdir(project_path)\n",
        "\n",
        "# Verify the current working directory and list files to confirm\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(\"Files in current directory:\")\n",
        "for item in os.listdir('.'):\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MZoO1xF_uZe"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# ðŸ”§ CONFIGURE YOUR EXPERIMENT HERE\n",
        "# ========================================================================\n",
        "DATASET = 'amazon'  # Options: 'yelp', 'amazon', 'gowalla', 'movielens'\n",
        "\n",
        "# Grid Search Configuration for Hard Negative Sampling\n",
        "GRID_SEARCH_ENABLED = True  # Set to False for single experiment\n",
        "HARD_NEG_SAMPLES_K = [3, 5, 7]  # Number of hard negatives to test\n",
        "CONTRASTIVE_WEIGHTS = [0.2, 0.1, 0.01, 0]  # Contrastive loss weights (Î»)\n",
        "GRID_SEARCH_EPOCHS = 25  # Epochs per grid search experiment\n",
        "\n",
        "# Single experiment configuration (used when GRID_SEARCH_ENABLED = False)\n",
        "SINGLE_K = 5\n",
        "SINGLE_LAMBDA = 0.1\n",
        "SINGLE_EPOCHS = 150\n",
        "\n",
        "# Google Drive results path\n",
        "DRIVE_RESULTS_PATH = '/content/drive/MyDrive/HardGNN_Results'  # Results will be saved here\n",
        "# ========================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asmqMdzI__Cz"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# CELL 1: Environment Setup - PRIORITIZING COLAB DEFAULTS for TF/NumPy\n",
        "# ========================================================================\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import site\n",
        "\n",
        "def install_missing_dependencies():\n",
        "    \"\"\"Install/upgrade non-ML core dependencies. NumPy and TensorFlow should be Colab's defaults.\"\"\"\n",
        "    print(\"Installing/upgrading non-ML core dependencies for HardGNN...\")\n",
        "    print(f\"Detected Python version: {sys.version}\")\n",
        "\n",
        "    # We rely on Colab's pre-installed versions.\n",
        "    dependencies = [\n",
        "        \"matplotlib>=3.5.0\",\n",
        "        \"scipy>=1.12.0\",\n",
        "        \"protobuf>=3.19.0,<4.25.0\",\n",
        "        \"pandas>=1.3.0\",\n",
        "        \"scikit-learn>=1.0.0\"\n",
        "    ]\n",
        "\n",
        "    print(\"Attempting to install/upgrade non-ML core dependencies to user site...\")\n",
        "    for dep in dependencies:\n",
        "        print(f\"Processing {dep}...\")\n",
        "        try:\n",
        "            command = [sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"--upgrade\", \"--user\", dep]\n",
        "            print(f\"   Executing: {' '.join(command)}\")\n",
        "            result = subprocess.run(command,\n",
        "                                  check=True, capture_output=True, text=True, timeout=180)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Warning: Could not process {dep}. Pip error: {e}\")\n",
        "        except subprocess.TimeoutExpired as e:\n",
        "            print(f\"Timeout: Processing of {dep} took too long.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred processing {dep}: {e}\")\n",
        "\n",
        "    print(\"Non-ML core dependency processing complete.\")\n",
        "    try:\n",
        "        # Ensure user site packages are in path\n",
        "        if hasattr(site, 'USER_SITE') and site.USER_SITE and site.USER_SITE not in sys.path:\n",
        "            sys.path.insert(0, site.USER_SITE)\n",
        "        # For Colab/Linux, also consider adding local/bin to PATH if it exists for any pip installed CLIs\n",
        "        local_bin_path = os.path.expanduser(\"~/.local/bin\")\n",
        "        if os.path.isdir(local_bin_path) and local_bin_path not in os.environ['PATH']:\n",
        "            os.environ['PATH'] = local_bin_path + os.pathsep + os.environ['PATH']\n",
        "    except Exception as e:\n",
        "        print(f\"Could not update sys.path/PATH for user site: {e}\")\n",
        "\n",
        "def setup_tensorflow_compatibility(tf_module, numpy_module):\n",
        "    print(f\"Using TensorFlow version: {tf_module.__version__ if tf_module else 'N/A'}\")\n",
        "    print(f\"Using NumPy version: {numpy_module.__version__ if numpy_module else 'N/A'}\")\n",
        "\n",
        "    if not tf_module or not numpy_module:\n",
        "        return False\n",
        "\n",
        "    # Informational checks about loaded versions\n",
        "    if numpy_module.__version__.startswith(\"2.\"):\n",
        "        print(f\"NumPy version is {numpy_module.__version__}. Colab's TensorFlow ({tf_module.__version__}) should be compatible (e.g., >=2.16 or specially built).\" )\n",
        "    elif numpy_module.__version__.startswith(\"1.\"):\n",
        "        print(f\"NumPy version is {numpy_module.__version__}. Colab's TensorFlow ({tf_module.__version__}) should be compatible (e.g., <=2.15 or specially built).\" )\n",
        "    else:\n",
        "        print(f\"Unknown NumPy version pattern: {numpy_module.__version__}\")\n",
        "\n",
        "    try:\n",
        "        import ml_dtypes\n",
        "        print(f\"ml_dtypes version found: {ml_dtypes.__version__} (from {ml_dtypes.__file__})\")\n",
        "        if numpy_module.__version__.startswith(\"2.\") and not ml_dtypes.__version__.startswith((\"0.4\", \"0.5\")):\n",
        "            print(f\"WARNING: ml_dtypes version ({ml_dtypes.__version__}) might not be ideal for NumPy 2.x (expected 0.4.x or 0.5.x). Check for runtime issues.\")\n",
        "        elif numpy_module.__version__.startswith(\"1.\") and ml_dtypes.__version__.startswith((\"0.4\", \"0.5\")):\n",
        "             print(f\"WARNING: ml_dtypes version ({ml_dtypes.__version__}) might not be ideal for NumPy 1.x (expected <0.4.x). Check for runtime issues.\")\n",
        "    except ImportError:\n",
        "        print(\"ml_dtypes not explicitly found or importable by script. TensorFlow might bundle it or manage it internally.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ml_dtypes check: {e}\")\n",
        "\n",
        "    try:\n",
        "        tf_module.compat.v1.disable_eager_execution()\n",
        "        tf_module.compat.v1.disable_v2_behavior()\n",
        "        gpus = tf_module.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            print(f\"GPU acceleration available: {len(gpus)} GPU(s) detected\")\n",
        "            for gpu_device in gpus:\n",
        "                print(f\"   - {gpu_device}\")\n",
        "                try:\n",
        "                    tf_module.config.experimental.set_memory_growth(gpu_device, True)\n",
        "                    print(f\"GPU memory growth configured for {gpu_device}\")\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Could not configure GPU memory growth for {gpu_device}: {e}\")\n",
        "        else:\n",
        "            print(\"No GPU detected, will use CPU.\")\n",
        "        return True\n",
        "    except AttributeError as e:\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "def verify_colab_environment(tf_module, numpy_module):\n",
        "    import sys\n",
        "    import psutil\n",
        "    import platform\n",
        "\n",
        "    print(f\"Python: {sys.version}\")\n",
        "    print(f\"sys.path (first few entries): {str(sys.path[:5])}\")\n",
        "    print(f\"Platform: {platform.platform()}\")\n",
        "    print(f\"Architecture: {platform.machine()}\")\n",
        "\n",
        "    if numpy_module:\n",
        "        print(f\"NumPy Version (loaded): {numpy_module.__version__}\")\n",
        "        print(f\"NumPy Path: {numpy_module.__file__}\")\n",
        "    else:\n",
        "        print(\"NumPy Version (loaded): NOT LOADED\")\n",
        "\n",
        "    if tf_module:\n",
        "        print(f\"TensorFlow Version (loaded): {tf_module.__version__}\")\n",
        "        print(f\"TensorFlow Path: {tf_module.__file__}\")\n",
        "    else:\n",
        "        print(\"TensorFlow Version (loaded): NOT LOADED\")\n",
        "\n",
        "    try:\n",
        "        import ml_dtypes\n",
        "        print(f\"ml_dtypes Version (loaded): {ml_dtypes.__version__} from {ml_dtypes.__file__}\")\n",
        "    except Exception:\n",
        "        print(f\"ml_dtypes: Not found by script or error during import check (may be internal to TF).\")\n",
        "\n",
        "    try:\n",
        "        import tensorflow_metadata\n",
        "        print(f\"tensorflow-metadata Version (loaded): {tensorflow_metadata.__version__} from {tensorflow_metadata.__file__}\")\n",
        "    except Exception:\n",
        "        print(f\"tensorflow-metadata: Not found by script or error (may not be needed or part of default TF).\")\n",
        "\n",
        "    memory = psutil.virtual_memory()\n",
        "    memory_gb = memory.total / (1024**3)\n",
        "    print(f\"Available RAM: {memory_gb:.1f} GB\")\n",
        "    disk = psutil.disk_usage('/')\n",
        "    disk_gb = disk.free / (1024**3)\n",
        "    print(f\"Available disk space: {disk_gb:.1f} GB\")\n",
        "    return True\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "print(\"=\" * 60)\n",
        "print(\"HardGNN Setup for Google Colab Pro+ (PRIORITIZING COLAB DEFAULTS for TF/NumPy)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Install/Upgrade other dependencies (non TF/NumPy)\n",
        "install_missing_dependencies()\n",
        "\n",
        "# 2. Import Colab's default NumPy and TensorFlow\n",
        "numpy_to_use = None\n",
        "tensorflow_to_use = None\n",
        "\n",
        "try:\n",
        "    import numpy\n",
        "    numpy_to_use = numpy\n",
        "except Exception as e:\n",
        "    print(\"This is a critical failure. Further steps will likely fail.\")\n",
        "\n",
        "try:\n",
        "    import tensorflow\n",
        "    tensorflow_to_use = tensorflow\n",
        "except Exception as e:\n",
        "    print(f\"This could be due to an underlying issue with its dependencies (like the loaded NumPy version) or Colab environment configuration.\")\n",
        "\n",
        "if not numpy_to_use or not tensorflow_to_use:\n",
        "    print(\"CRITICAL FAILURE: Could not import Colab's default NumPy or TensorFlow. Environment setup will likely be incomplete or fail.\")\n",
        "\n",
        "# 3. Setup TensorFlow compatibility using the imported Colab modules\n",
        "setup_successful = False # Default to False\n",
        "if numpy_to_use and tensorflow_to_use:\n",
        "    setup_successful = setup_tensorflow_compatibility(tf_module=tensorflow_to_use, numpy_module=numpy_to_use)\n",
        "else:\n",
        "    print(\"Skipping TensorFlow compatibility setup as core modules (NumPy/TensorFlow) failed to load.\")\n",
        "\n",
        "# 4. Verify environment using the imported Colab modules\n",
        "verify_colab_environment(tf_module=tensorflow_to_use, numpy_module=numpy_to_use)\n",
        "\n",
        "if not setup_successful:\n",
        "    # Custom error message based on whether TF/NumPy even loaded\n",
        "    if not numpy_to_use or not tensorflow_to_use:\n",
        "        raise RuntimeError(\"TensorFlow/NumPy native import failed. Cannot configure environment.\")\n",
        "    else:\n",
        "        raise RuntimeError(\"TensorFlow setup failed using Colab's default versions. There might be an incompatibility within the pre-built Colab environment, or the TF1 compatibility layer cannot be applied to the loaded versions.\")\n",
        "\n",
        "print(\"Environment setup attempt complete using Colab's default TF/NumPy (or best effort)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv6APJWeADjC"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# CELL 2: Dataset Configuration and Module Import\n",
        "# ========================================================================\n",
        "if tensorflow_to_use:\n",
        "    tf = tensorflow_to_use\n",
        "else:\n",
        "    import tensorflow as tf\n",
        "    print(\"Warning: tensorflow_to_use was not set from Cell 1. Attempted direct import of tensorflow as tf.\")\n",
        "\n",
        "# Core Python Standard Library imports\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import gc\n",
        "import traceback\n",
        "import queue\n",
        "import threading\n",
        "import multiprocessing as mp\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Import our modules\n",
        "from Params import args\n",
        "import Utils.TimeLogger as logger\n",
        "from Utils.TimeLogger import log\n",
        "from DataHandler import DataHandler\n",
        "\n",
        "# Import the HardGNN model\n",
        "print(\"Importing HardGNN model...\")\n",
        "try:\n",
        "    from HardGNN_model import Recommender\n",
        "    print(\"Successfully imported HardGNN model\")\n",
        "except ImportError as e:\n",
        "    print(f\"Failed to import HardGNN model: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "def configure_dataset(dataset_name, hard_neg_k=5, contrastive_weight=0.1):\n",
        "    \"\"\"Configure parameters based on validated configurations for each dataset\"\"\"\n",
        "\n",
        "    # Set base dataset\n",
        "    args.data = dataset_name.lower()\n",
        "\n",
        "    # Dataset-specific validated configurations\n",
        "    if dataset_name.lower() == 'yelp':\n",
        "        args.lr = 1e-3\n",
        "        args.reg = 1e-2\n",
        "        args.temp = 0.1\n",
        "        args.ssl_reg = 1e-7\n",
        "        args.epoch = 150\n",
        "        args.batch = 512\n",
        "        args.sslNum = 40\n",
        "        args.graphNum = 12\n",
        "        args.gnn_layer = 3\n",
        "        args.att_layer = 2\n",
        "        args.testSize = 1000\n",
        "        args.ssldim = 32\n",
        "        args.sampNum = 40\n",
        "\n",
        "    elif dataset_name.lower() == 'amazon':\n",
        "        args.lr = 1e-3\n",
        "        args.reg = 1e-2\n",
        "        args.temp = 0.1\n",
        "        args.ssl_reg = 1e-6\n",
        "        args.epoch = 150\n",
        "        args.batch = 512\n",
        "        args.sslNum = 80\n",
        "        args.graphNum = 5\n",
        "        args.pred_num = 0\n",
        "        args.gnn_layer = 3\n",
        "        args.att_layer = 4\n",
        "        args.testSize = 1000\n",
        "        args.keepRate = 0.5\n",
        "        args.sampNum = 40\n",
        "        args.pos_length = 200\n",
        "\n",
        "    elif dataset_name.lower() == 'gowalla':\n",
        "        args.lr = 2e-3\n",
        "        args.reg = 1e-2\n",
        "        args.temp = 0.1\n",
        "        args.ssl_reg = 1e-6\n",
        "        args.epoch = 150\n",
        "        args.batch = 512\n",
        "        args.graphNum = 3\n",
        "        args.gnn_layer = 2\n",
        "        args.att_layer = 1\n",
        "        args.testSize = 1000\n",
        "        args.sampNum = 40\n",
        "\n",
        "    elif dataset_name.lower() == 'movielens':\n",
        "        args.lr = 1e-3\n",
        "        args.reg = 1e-2\n",
        "        args.ssl_reg = 1e-6\n",
        "        args.epoch = 150\n",
        "        args.batch = 512\n",
        "        args.sampNum = 40\n",
        "        args.sslNum = 90\n",
        "        args.graphNum = 6\n",
        "        args.gnn_layer = 2\n",
        "        args.att_layer = 3\n",
        "        args.testSize = 1000\n",
        "        args.ssldim = 48\n",
        "        args.keepRate = 0.5\n",
        "        args.pos_length = 200\n",
        "        args.leaky = 0.5\n",
        "\n",
        "    else:\n",
        "        print(\"Available datasets: yelp, amazon, gowalla, movielens\")\n",
        "\n",
        "    # Handle edge cases for hard negative sampling and contrastive loss\n",
        "    if hard_neg_k == 0:\n",
        "        # K=0: Disable hard negative sampling entirely\n",
        "        args.use_hard_neg = False\n",
        "        args.hard_neg_top_k = 0\n",
        "    else:\n",
        "        # K>0: Enable hard negative sampling\n",
        "        args.use_hard_neg = True\n",
        "        args.hard_neg_top_k = hard_neg_k\n",
        "\n",
        "    # Set contrastive weight (Î»=0 is handled in model during loss computation)\n",
        "    args.contrastive_weight = contrastive_weight\n",
        "    # Note: Ï„ (temperature) is already set in args.temp = 0.1\n",
        "\n",
        "    # Performance optimization for hard negative sampling\n",
        "    args.cache_refresh_steps = 25  # Refresh embeddings every N training steps\n",
        "\n",
        "    # Enable AMP to be handled within the model\n",
        "    args.enable_amp = True # This will be read by HardGNN_model.py\n",
        "\n",
        "    args.tstEpoch = 3  # Test every 3 epochs (can be adjusted if needed for full runs)\n",
        "\n",
        "    # Set save path\n",
        "    args.save_path = f'hardgnn_{dataset_name.lower()}_k{hard_neg_k}_lambda{contrastive_weight}'\n",
        "\n",
        "    return args\n",
        "\n",
        "# Configure the dataset\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    # Use first combination for initial setup\n",
        "    configure_dataset(DATASET, HARD_NEG_SAMPLES_K[0], CONTRASTIVE_WEIGHTS[0])\n",
        "    print(f\"   K values: {HARD_NEG_SAMPLES_K}\")\n",
        "    print(f\"   Î» values: {CONTRASTIVE_WEIGHTS}\")\n",
        "    print(f\"   Results will be saved to: {DRIVE_RESULTS_PATH}\")\n",
        "else:\n",
        "    configure_dataset(DATASET, SINGLE_K, SINGLE_LAMBDA)\n",
        "    args.epoch = SINGLE_EPOCHS\n",
        "\n",
        "# Initialize grid search results storage\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    grid_search_results = []\n",
        "    # Create results directory in Google Drive\n",
        "    os.makedirs(DRIVE_RESULTS_PATH, exist_ok=True)\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    print(f\"  Mode: Grid Search\")\n",
        "    print(f\"  K range: {HARD_NEG_SAMPLES_K}\")\n",
        "    print(f\"  Î» range: {CONTRASTIVE_WEIGHTS}\")\n",
        "else:\n",
        "    print(f\"  Mode: Single Experiment\")\n",
        "    print(f\"  Hard Negatives (K): {args.hard_neg_top_k}\")\n",
        "    print(f\"  Contrastive Weight (Î»): {args.contrastive_weight}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaJWjmOJAR_p"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# CELL 3: Load Dataset\n",
        "# ========================================================================\n",
        "\n",
        "# Initialize and load data\n",
        "logger.saveDefault = True\n",
        "log(f'Starting {DATASET} data loading...')\n",
        "\n",
        "handler = DataHandler()\n",
        "handler.LoadData()\n",
        "\n",
        "log(f'{DATASET} data loaded successfully')\n",
        "print(f\"{DATASET.upper()} Dataset Statistics:\")\n",
        "print(f\"  Users: {args.user:,}\")\n",
        "print(f\"  Items: {args.item:,}\")\n",
        "print(f\"  Training interactions: {handler.trnMat.nnz:,}\")\n",
        "print(f\"  Test users: {len(handler.tstUsrs):,}\")\n",
        "print(f\"  Time-based graphs: {len(handler.subMat)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tFwxi20AZkV"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# CELL 4: Validate Contrastive Loss Component\n",
        "# ========================================================================\n",
        "\n",
        "print(f\" Validating Hard Negative Sampling on {DATASET}...\")\n",
        "print(f\" Testing with Ï„={args.temp}, K={args.hard_neg_top_k}, Î»={args.contrastive_weight}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.compat.v1.set_random_seed(42)\n",
        "\n",
        "# Initialize TensorFlow session with GPU config\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n",
        "\n",
        "with tf.compat.v1.Session(config=config) as sess:\n",
        "    # Initialize HardGNN model\n",
        "    model = Recommender(sess, handler)\n",
        "    model.prepareModel()\n",
        "\n",
        "    # Initialize variables\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    log(' Model initialized (random weights)')\n",
        "\n",
        "    # Test contrastive loss on a small batch\n",
        "    test_users = handler.tstUsrs[:32]  # Small batch for validation\n",
        "\n",
        "    try:\n",
        "        # Sample batch with hard negatives\n",
        "        uLocs, iLocs, sequence, mask, uLocs_seq = model.sampleTrainBatch(\n",
        "            test_users, handler.trnMat, handler.timeMat, train_sample_num=10\n",
        "        )\n",
        "\n",
        "        # Sample SSL batch\n",
        "        suLocs, siLocs, suLocs_seq = model.sampleSslBatch(test_users, handler.subMat, False)\n",
        "\n",
        "        # Prepare feed dict\n",
        "        feed_dict = {\n",
        "            model.uids: uLocs,\n",
        "            model.iids: iLocs,\n",
        "            model.sequence: sequence,\n",
        "            model.mask: mask,\n",
        "            model.is_train: False,\n",
        "            model.uLocs_seq: uLocs_seq,\n",
        "            model.keepRate: 1.0\n",
        "        }\n",
        "\n",
        "        for k in range(args.graphNum):\n",
        "            feed_dict[model.suids[k]] = suLocs[k]\n",
        "            feed_dict[model.siids[k]] = siLocs[k]\n",
        "            feed_dict[model.suLocs_seq[k]] = suLocs_seq[k]\n",
        "\n",
        "        # Run forward pass\n",
        "        if hasattr(model, 'contrastive_loss'):\n",
        "            results = sess.run([\n",
        "                model.contrastive_loss,\n",
        "                model.preLoss,\n",
        "                model.posPred,\n",
        "                model.negPred\n",
        "            ], feed_dict=feed_dict)\n",
        "\n",
        "            contrastive_loss, pre_loss, pos_pred, neg_pred = results\n",
        "\n",
        "            print(f\"ðŸŽ¯ HARD NEGATIVE SAMPLING VALIDATION - {DATASET.upper()}\")\n",
        "            print(f\"ðŸ“Š Metrics:\")\n",
        "            print(f\"  Contrastive Loss: {contrastive_loss:.6f}\")\n",
        "            print(f\"  Supervised Loss: {pre_loss:.6f}\")\n",
        "            print(f\"  Positive Predictions: {np.mean(pos_pred):.4f} Â± {np.std(pos_pred):.4f}\")\n",
        "            print(f\"  Negative Predictions: {np.mean(neg_pred):.4f} Â± {np.std(neg_pred):.4f}\")\n",
        "            print(f\"  Prediction Gap: {np.mean(pos_pred) - np.mean(neg_pred):.4f}\")\n",
        "\n",
        "            if np.mean(pos_pred) > np.mean(neg_pred):\n",
        "                print(\"  Positive predictions > Negative predictions\")\n",
        "            else:\n",
        "                print(\"  Negative predictions >= Positive predictions\")\n",
        "\n",
        "            if contrastive_loss > 0 and not np.isnan(contrastive_loss):\n",
        "                print(\"  Hard negative sampling working correctly\")\n",
        "            else:\n",
        "                print(\"  Issue with hard negative sampling\")\n",
        "\n",
        "            print(f\"\\n Validation Complete - Ready for {DATASET.upper()} Training!\")\n",
        "        else:\n",
        "            print(\" Hard negative sampling not available\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Validation error: {e}\")\n",
        "        print(\"Proceeding with training...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cdkjAG6cAdAh"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# CELL 5: Grid Search Training or Single Experiment\n",
        "# ========================================================================\n",
        "\n",
        "def save_experiment_result_to_drive(result, dataset_name, experiment_num=None):\n",
        "    \"\"\"Save individual experiment result to Google Drive\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    if experiment_num is not None:\n",
        "        filename = f\"hardgnn_{dataset_name}_exp{experiment_num:02d}_k{result['k_value']}_lambda{result['lambda_value']}_{timestamp}.json\"\n",
        "    else:\n",
        "        filename = f\"hardgnn_{dataset_name}_single_k{result['k_value']}_lambda{result['lambda_value']}_{timestamp}.json\"\n",
        "\n",
        "    filepath = os.path.join(DRIVE_RESULTS_PATH, filename)\n",
        "\n",
        "    # Save detailed result\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(result, f, indent=2, default=str)\n",
        "\n",
        "    print(f\" Result saved to Drive: {filename}\")\n",
        "    return filepath\n",
        "\n",
        "def save_grid_search_summary_to_drive(all_results, dataset_name):\n",
        "    \"\"\"Save complete grid search summary to Google Drive\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Save JSON summary\n",
        "    json_filename = f\"hardgnn_{dataset_name}_grid_search_summary_{timestamp}.json\"\n",
        "    json_filepath = os.path.join(DRIVE_RESULTS_PATH, json_filename)\n",
        "\n",
        "    with open(json_filepath, 'w') as f:\n",
        "        json.dump({\n",
        "            'dataset': dataset_name,\n",
        "            'grid_search_config': {\n",
        "                'k_values': HARD_NEG_SAMPLES_K,\n",
        "                'lambda_values': CONTRASTIVE_WEIGHTS,\n",
        "                'epochs_per_experiment': GRID_SEARCH_EPOCHS\n",
        "            },\n",
        "            'results': all_results,\n",
        "            'timestamp': timestamp\n",
        "        }, f, indent=2, default=str)\n",
        "\n",
        "    # Save CSV summary\n",
        "    csv_filename = f\"hardgnn_{dataset_name}_grid_search_summary_{timestamp}.csv\"\n",
        "    csv_filepath = os.path.join(DRIVE_RESULTS_PATH, csv_filename)\n",
        "\n",
        "    df = pd.DataFrame(all_results)\n",
        "    df.to_csv(csv_filepath, index=False)\n",
        "\n",
        "    print(f\" Grid search summary saved to Drive:\")\n",
        "    print(f\"   JSON: {json_filename}\")\n",
        "    print(f\"   CSV: {csv_filename}\")\n",
        "\n",
        "    return json_filepath, csv_filepath\n",
        "\n",
        "def run_single_experiment(k_value, lambda_value, epochs, experiment_num=None, total_experiments=None):\n",
        "    \"\"\"Run a single experiment with given hyperparameters - GPU optimized\"\"\"\n",
        "\n",
        "    # Reset TensorFlow graph for fresh start - THIS MUST BE FIRST\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "\n",
        "    # Also reset NNLayers_tf2 global parameter tracking\n",
        "    from Utils import NNLayers_tf2\n",
        "    NNLayers_tf2.reset_nn_params()\n",
        "\n",
        "    # Configure for this experiment (this will set args.enable_amp = True)\n",
        "    configure_dataset(DATASET, k_value, lambda_value)\n",
        "    args.epoch = epochs\n",
        "\n",
        "    # Create experiment identifier\n",
        "    exp_id = f\"K{k_value}_Î»{lambda_value}\"\n",
        "    if experiment_num is not None:\n",
        "        exp_header = f\"Experiment {experiment_num}/{total_experiments}: {exp_id}\"\n",
        "    else:\n",
        "        exp_header = f\"Experiment: {exp_id}\"\n",
        "\n",
        "    print(f\"{exp_header}\")\n",
        "    print(f\"Configuration: K={k_value}, Î»={lambda_value}, Epochs={epochs}\")\n",
        "\n",
        "    # Determine experiment type for edge case documentation\n",
        "    if k_value == 0 and lambda_value == 0:\n",
        "        experiment_type = \"Pure SelfGNN (no hard negatives, no contrastive loss)\"\n",
        "    elif k_value == 0 and lambda_value > 0:\n",
        "        experiment_type = \"SelfGNN + Contrastive only (no hard negatives)\"\n",
        "    elif k_value > 0 and lambda_value == 0:\n",
        "        experiment_type = \"SelfGNN + Hard negatives only (no contrastive loss)\"\n",
        "    else:\n",
        "        experiment_type = \"Full HardGNN (hard negatives + contrastive loss)\"\n",
        "\n",
        "    print(f\"   Experiment Type: {experiment_type}\")\n",
        "    print(f\"   Hard Negative Sampling: {'Disabled' if k_value == 0 else 'Enabled'}\")\n",
        "    print(f\"   Contrastive Loss: {'Disabled (Î»=0)' if lambda_value == 0 else f'Enabled (Î»={lambda_value})'}\")\n",
        "\n",
        "    # GPU-optimized TensorFlow session configuration\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.allow_soft_placement = True\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.95  # Use most of GPU memory\n",
        "\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        config.graph_options.optimizer_options.global_jit_level = tf.compat.v1.OptimizerOptions.ON_1\n",
        "        # config.gpu_options.experimental.enable_async_io = True # Ensure this is commented out or removed\n",
        "        log_message_parts = [\" GPU optimization enabled: XLA JIT\", \"95% memory allocation\"]\n",
        "        # AMP status is now logged by the model, but we can mention it's attempted.\n",
        "        log_message_parts.append(\"AMP attempted in model\")\n",
        "        print(f\"{', '.join(log_message_parts)}. Async I/O removed.\")\n",
        "\n",
        "    # Enable intra-op and inter-op parallelism for CPU efficiency\n",
        "    config.intra_op_parallelism_threads = mp.cpu_count()\n",
        "    config.inter_op_parallelism_threads = mp.cpu_count() // 2\n",
        "    config.use_per_session_threads = True\n",
        "\n",
        "    experiment_start_time = datetime.now()\n",
        "\n",
        "    sess = None # Initialize sess to None for the finally block\n",
        "    try:\n",
        "        with tf.compat.v1.Session(config=config) as sess:\n",
        "            # Initialize model\n",
        "            model = Recommender(sess, handler)\n",
        "            model.prepareModel()\n",
        "\n",
        "            # Initialize variables\n",
        "            init = tf.compat.v1.global_variables_initializer()\n",
        "            sess.run(init)\n",
        "\n",
        "            # Training tracking\n",
        "            best_hr = 0.0\n",
        "            best_ndcg = 0.0\n",
        "            best_epoch = 0\n",
        "            best_results = {}\n",
        "            final_hr = 0.0\n",
        "            final_ndcg = 0.0\n",
        "            final_epoch = epochs\n",
        "            final_results = {}\n",
        "            epoch_results = []\n",
        "\n",
        "            # Pipeline optimization: Pre-compute user batches for parallel processing\n",
        "            print(f\" Optimizing data pipeline for parallel processing...\")\n",
        "            num_users = args.user\n",
        "            batch_size = args.batch\n",
        "            user_batches = []\n",
        "\n",
        "            # Pre-generate all user ID permutations for faster training\n",
        "            for ep in range(epochs):\n",
        "                sfIds = np.random.permutation(num_users)[:args.trnNum]\n",
        "                epoch_batches = []\n",
        "                steps = int(np.ceil(len(sfIds) / batch_size))\n",
        "                for i in range(steps):\n",
        "                    st = i * batch_size\n",
        "                    ed = min((i+1) * batch_size, len(sfIds))\n",
        "                    epoch_batches.append(sfIds[st:ed])\n",
        "                user_batches.append(epoch_batches)\n",
        "\n",
        "            # Training loop with pipeline optimization\n",
        "            for ep in range(epochs):\n",
        "                test = (ep % args.tstEpoch == 0)\n",
        "\n",
        "                if ep % 5 == 0 or test:\n",
        "                    print(f\" Epoch {ep+1}/{epochs} (K={k_value}, Î»={lambda_value}) - GPU Pipeline Active\")\n",
        "\n",
        "                # GPU-optimized training with pre-computed batches\n",
        "                train_results = train_epoch_optimized(model, user_batches[ep], sess)\n",
        "\n",
        "                # Test if it's a test epoch\n",
        "                if test:\n",
        "                    test_results = model.testEpoch()\n",
        "                    hr = test_results['HR']\n",
        "                    ndcg = test_results['NDCG']\n",
        "\n",
        "                    # Track best results\n",
        "                    if ndcg > best_ndcg:\n",
        "                        best_ndcg = ndcg\n",
        "                        best_hr = hr\n",
        "                        best_epoch = ep + 1  # Convert to 1-indexed\n",
        "                        best_results = test_results.copy()\n",
        "                        print(f\" New best: HR={hr:.4f}, NDCG={ndcg:.4f} (Epoch {ep+1})\")\n",
        "\n",
        "                    # Store epoch results\n",
        "                    epoch_results.append({\n",
        "                        'epoch': ep+1,\n",
        "                        'hr': hr,\n",
        "                        'ndcg': ndcg,\n",
        "                        'train_loss': train_results.get('Loss', 0),\n",
        "                        'pre_loss': train_results.get('preLoss', 0),\n",
        "                        'contrastive_loss': train_results.get('contrastiveLoss', 0)\n",
        "                    })\n",
        "\n",
        "            # Final evaluation\n",
        "            final_test_results = model.testEpoch()\n",
        "            final_hr = final_test_results['HR']\n",
        "            final_ndcg = final_test_results['NDCG']\n",
        "            final_results = final_test_results.copy()\n",
        "            experiment_duration = (datetime.now() - experiment_start_time).total_seconds()\n",
        "\n",
        "            # ========================================================================\n",
        "            # COMPREHENSIVE RESULTS PRINTING AND LOGGING\n",
        "            # ========================================================================\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(f\" EXPERIMENT RESULTS: {exp_id}\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\" Experiment Type: {experiment_type}\")\n",
        "            print(f\"  Configuration:\")\n",
        "            print(f\"   â€¢ K (Hard negatives): {k_value}\")\n",
        "            print(f\"   â€¢ Î» (Contrastive weight): {lambda_value}\")\n",
        "            print(f\"   â€¢ Epochs: {epochs}\")\n",
        "            print(f\"   â€¢ Duration: {experiment_duration/60:.1f} minutes\")\n",
        "            print(f\"   â€¢ GPU Efficiency: {epochs*len(user_batches[0])/(experiment_duration/60):.1f} batches/min\")\n",
        "            print()\n",
        "\n",
        "            print(f\" BEST PERFORMANCE:\")\n",
        "            print(f\"   â€¢ Best HR: {best_hr:.4f}\")\n",
        "            print(f\"   â€¢ Best NDCG: {best_ndcg:.4f}\")\n",
        "            print(f\"   â€¢ Best Epoch: {best_epoch}\")\n",
        "            if 'HR5' in best_results:\n",
        "                print(f\"   â€¢ Best HR@5: {best_results['HR5']:.4f}\")\n",
        "                print(f\"   â€¢ Best NDCG@5: {best_results['NDCG5']:.4f}\")\n",
        "            if 'HR20' in best_results:\n",
        "                print(f\"   â€¢ Best HR@20: {best_results['HR20']:.4f}\")\n",
        "                print(f\"   â€¢ Best NDCG@20: {best_results['NDCG20']:.4f}\")\n",
        "            print()\n",
        "\n",
        "            print(f\" FINAL PERFORMANCE (Epoch {final_epoch}):\")\n",
        "            print(f\"   â€¢ Final HR: {final_hr:.4f}\")\n",
        "            print(f\"   â€¢ Final NDCG: {final_ndcg:.4f}\")\n",
        "            if 'HR5' in final_results:\n",
        "                print(f\"   â€¢ Final HR@5: {final_results['HR5']:.4f}\")\n",
        "                print(f\"   â€¢ Final NDCG@5: {final_results['NDCG5']:.4f}\")\n",
        "            if 'HR20' in final_results:\n",
        "                print(f\"   â€¢ Final HR@20: {final_results['HR20']:.4f}\")\n",
        "                print(f\"   â€¢ Final NDCG@20: {final_results['NDCG20']:.4f}\")\n",
        "            print()\n",
        "\n",
        "            # Performance comparison\n",
        "            hr_improvement = ((final_hr - best_hr) / best_hr * 100) if best_hr > 0 else 0\n",
        "            ndcg_improvement = ((final_ndcg - best_ndcg) / best_ndcg * 100) if best_ndcg > 0 else 0\n",
        "            print(f\"ðŸ“ˆ PERFORMANCE ANALYSIS:\")\n",
        "            print(f\"   â€¢ HR improvement (final vs best): {hr_improvement:+.2f}%\")\n",
        "            print(f\"   â€¢ NDCG improvement (final vs best): {ndcg_improvement:+.2f}%\")\n",
        "\n",
        "            if best_epoch < final_epoch:\n",
        "                print(f\"   â€¢ Best performance at epoch {best_epoch}, final at epoch {final_epoch}\")\n",
        "                print(f\"   â€¢ Potential overfitting detected ({final_epoch - best_epoch} epochs after best)\")\n",
        "            else:\n",
        "                print(f\"   â€¢ Best performance maintained until final epoch\")\n",
        "\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            # Comprehensive experiment result for logging\n",
        "            experiment_result = {\n",
        "                'dataset': DATASET,\n",
        "                'experiment_type': experiment_type,\n",
        "                'k_value': k_value,\n",
        "                'lambda_value': lambda_value,\n",
        "                'epochs': epochs,\n",
        "                'duration_minutes': experiment_duration/60,\n",
        "                'gpu_efficiency_batches_per_min': epochs*len(user_batches[0])/(experiment_duration/60) if experiment_duration > 0 else 0,\n",
        "\n",
        "                # Best performance metrics\n",
        "                'best_hr': best_hr,\n",
        "                'best_ndcg': best_ndcg,\n",
        "                'best_epoch': best_epoch,\n",
        "                'best_hr5': best_results.get('HR5', None),\n",
        "                'best_ndcg5': best_results.get('NDCG5', None),\n",
        "                'best_hr20': best_results.get('HR20', None),\n",
        "                'best_ndcg20': best_results.get('NDCG20', None),\n",
        "\n",
        "                # Final performance metrics\n",
        "                'final_hr': final_hr,\n",
        "                'final_ndcg': final_ndcg,\n",
        "                'final_epoch': final_epoch,\n",
        "                'final_hr5': final_results.get('HR5', None),\n",
        "                'final_ndcg5': final_results.get('NDCG5', None),\n",
        "                'final_hr20': final_results.get('HR20', None),\n",
        "                'final_ndcg20': final_results.get('NDCG20', None),\n",
        "\n",
        "                # Performance analysis\n",
        "                'hr_improvement_percent': hr_improvement,\n",
        "                'ndcg_improvement_percent': ndcg_improvement,\n",
        "                'potential_overfitting': best_epoch < final_epoch,\n",
        "                'epochs_after_best': max(0, final_epoch - best_epoch),\n",
        "\n",
        "                # Technical details\n",
        "                'hard_negatives_enabled': k_value > 0,\n",
        "                'contrastive_loss_enabled': lambda_value > 0,\n",
        "                'timestamp': experiment_start_time.isoformat(),\n",
        "\n",
        "                # Detailed epoch-by-epoch results\n",
        "                'epoch_details': epoch_results,\n",
        "\n",
        "                # Full results objects for reference\n",
        "                'best_results_full': best_results,\n",
        "                'final_results_full': final_results\n",
        "            }\n",
        "\n",
        "            print(f\" Experiment completed successfully!\")\n",
        "            print(f\" Saving detailed results to Google Drive...\")\n",
        "\n",
        "            # Save individual result to Google Drive\n",
        "            save_experiment_result_to_drive(experiment_result, DATASET, experiment_num)\n",
        "\n",
        "            return experiment_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Experiment {exp_id} failed: {e}\")\n",
        "        error_trace = traceback.format_exc()\n",
        "        print(f\"Traceback:\\\\n{error_trace}\")\n",
        "        # Log partial or error state if necessary\n",
        "        return {\n",
        "            \"experiment_id\": exp_id,\n",
        "            \"k_value\": k_value,\n",
        "            \"lambda_value\": lambda_value,\n",
        "            \"epochs_configured\": epochs,\n",
        "            \"status\": \"Failed\",\n",
        "            \"error_message\": str(e),\n",
        "            \"error_traceback\": error_trace,\n",
        "            \"duration_seconds\": (datetime.now() - experiment_start_time).total_seconds(),\n",
        "            \"experiment_type\": experiment_type,\n",
        "        }\n",
        "    finally:\n",
        "        if sess is not None:\n",
        "            sess.close()\n",
        "        # Force garbage collection to free up memory, especially GPU memory\n",
        "        gc.collect()\n",
        "\n",
        "def train_epoch_optimized(model, batch_list, sess):\n",
        "    \"\"\"GPU-optimized training epoch with pipeline parallelization\"\"\"\n",
        "    epochLoss, epochPreLoss = [0] * 2\n",
        "    epochContrastiveLoss = 0\n",
        "    sample_num_list = [40]\n",
        "    steps = len(batch_list)\n",
        "\n",
        "    # Pre-allocate arrays for better memory efficiency\n",
        "    batch_queue = queue.Queue(maxsize=3)  # Small buffer for pipeline overlap\n",
        "\n",
        "    def data_loader_worker():\n",
        "        \"\"\"Background thread for data loading pipeline\"\"\"\n",
        "        for s in range(len(sample_num_list)):\n",
        "            for i, batIds in enumerate(batch_list):\n",
        "                try:\n",
        "                    # Pre-compute batch data\n",
        "                    uLocs, iLocs, sequence, mask, uLocs_seq = model.sampleTrainBatch(\n",
        "                        batIds, model.handler.trnMat, model.handler.timeMat, sample_num_list[s]\n",
        "                    )\n",
        "                    suLocs, siLocs, suLocs_seq = model.sampleSslBatch(\n",
        "                        batIds, model.handler.subMat, False\n",
        "                    )\n",
        "\n",
        "                    batch_data = {\n",
        "                        'feed_dict': {\n",
        "                            model.uids: uLocs,\n",
        "                            model.iids: iLocs,\n",
        "                            model.sequence: sequence,\n",
        "                            model.mask: mask,\n",
        "                            model.is_train: True,\n",
        "                            model.uLocs_seq: uLocs_seq,\n",
        "                            model.keepRate: args.keepRate\n",
        "                        },\n",
        "                        'step_info': (i, s, steps, len(sample_num_list))\n",
        "                    }\n",
        "\n",
        "                    # Add SSL data\n",
        "                    for k in range(args.graphNum):\n",
        "                        batch_data['feed_dict'][model.suids[k]] = suLocs[k]\n",
        "                        batch_data['feed_dict'][model.siids[k]] = siLocs[k]\n",
        "                        batch_data['feed_dict'][model.suLocs_seq[k]] = suLocs_seq[k]\n",
        "\n",
        "                    batch_queue.put(batch_data, timeout=30)\n",
        "                except queue.Full:\n",
        "                    print(\" Data pipeline congestion - GPU processing slower than data loading\")\n",
        "                    batch_queue.put(batch_data, timeout=60)  # Wait longer\n",
        "                except Exception as e:\n",
        "                    print(f\" Data loading error: {e}\")\n",
        "                    break\n",
        "\n",
        "        # Signal completion\n",
        "        batch_queue.put(None)\n",
        "\n",
        "    # Start background data loading\n",
        "    data_thread = threading.Thread(target=data_loader_worker, daemon=True)\n",
        "    data_thread.start()\n",
        "\n",
        "    # GPU processing loop\n",
        "    processed_batches = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            batch_data = batch_queue.get(timeout=60)\n",
        "            if batch_data is None:  # Completion signal\n",
        "                break\n",
        "\n",
        "            feed_dict = batch_data['feed_dict']\n",
        "            i, s, steps, sample_num_list_len = batch_data['step_info']\n",
        "\n",
        "            # Determine target operations based on hard negative usage\n",
        "            if args.use_hard_neg:\n",
        "                target = [model.optimizer, model.preLoss, model.regLoss, model.loss,\n",
        "                         model.contrastive_loss, model.posPred, model.negPred, model.preds_one]\n",
        "            else:\n",
        "                target = [model.optimizer, model.preLoss, model.regLoss, model.loss,\n",
        "                         model.posPred, model.negPred, model.preds_one]\n",
        "\n",
        "            # GPU execution\n",
        "            res = sess.run(target, feed_dict=feed_dict)\n",
        "\n",
        "            if args.use_hard_neg:\n",
        "                preLoss, regLoss, loss, contrastiveLoss, pos, neg, pone = res[1:]\n",
        "                epochContrastiveLoss += contrastiveLoss\n",
        "                if processed_batches % 10 == 0:  # Less frequent logging for efficiency\n",
        "                    log('Step %d/%d: preloss = %.2f, REGLoss = %.2f, ConLoss = %.4f' %\n",
        "                        (i+s*steps, steps*sample_num_list_len, preLoss, regLoss, contrastiveLoss),\n",
        "                        save=False, oneline=True)\n",
        "            else:\n",
        "                preLoss, regLoss, loss, pos, neg, pone = res[1:]\n",
        "                if processed_batches % 10 == 0:\n",
        "                    log('Step %d/%d: preloss = %.2f, REGLoss = %.2f' %\n",
        "                        (i+s*steps, steps*sample_num_list_len, preLoss, regLoss),\n",
        "                        save=False, oneline=True)\n",
        "\n",
        "            epochLoss += loss\n",
        "            epochPreLoss += preLoss\n",
        "            processed_batches += 1\n",
        "\n",
        "        except queue.Empty:\n",
        "            print(\" GPU waiting for data - pipeline bottleneck detected\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\" GPU processing error: {e}\")\n",
        "            break\n",
        "\n",
        "    # Wait for data loading thread to complete\n",
        "    data_thread.join(timeout=10)\n",
        "\n",
        "    ret = dict()\n",
        "    ret['Loss'] = epochLoss / processed_batches if processed_batches > 0 else 0\n",
        "    ret['preLoss'] = epochPreLoss / processed_batches if processed_batches > 0 else 0\n",
        "    if args.use_hard_neg:\n",
        "        ret['contrastiveLoss'] = epochContrastiveLoss / processed_batches if processed_batches > 0 else 0\n",
        "\n",
        "    return ret\n",
        "\n",
        "def run_parallel_grid_search(k_values, lambda_values, epochs, dataset_name):\n",
        "    \"\"\"\n",
        "    GPU-optimized parallel grid search - Sequential experiments with pipeline optimization\n",
        "\n",
        "    Note: True parallelization of TensorFlow GPU experiments is limited by:\n",
        "    1. GPU memory constraints (each experiment needs ~95% of GPU memory)\n",
        "    2. TensorFlow session conflicts with concurrent GPU access\n",
        "    3. Google Colab single-GPU environment\n",
        "\n",
        "    Instead, we optimize:\n",
        "    - Pipeline parallelization within each experiment\n",
        "    - Data loading parallelization\n",
        "    - GPU utilization optimization\n",
        "    - Batch processing efficiency\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n GPU-OPTIMIZED GRID SEARCH on {dataset_name.upper()}\")\n",
        "    print(\"ðŸ”§ Optimization Strategy:\")\n",
        "    print(\"   â€¢ Sequential experiments (GPU memory constraint)\")\n",
        "    print(\"   â€¢ Pipeline parallelization within experiments\")\n",
        "    print(\"   â€¢ Background data loading threads\")\n",
        "    print(\"   â€¢ XLA JIT compilation + mixed precision\")\n",
        "    print(\"   â€¢ Async GPU I/O + optimized memory allocation\")\n",
        "    print(\"   â€¢ CPU parallelization for data processing\")\n",
        "\n",
        "    total_experiments = len(k_values) * len(lambda_values)\n",
        "    print(f\"Total combinations: {len(k_values)} K Ã— {len(lambda_values)} Î» = {total_experiments}\")\n",
        "\n",
        "    all_results = []\n",
        "    experiment_num = 0\n",
        "\n",
        "    # GPU warming: Pre-compile TensorFlow operations\n",
        "    configure_dataset(dataset_name, k_values[0], lambda_values[0])\n",
        "\n",
        "    # Sequential execution with maximum GPU optimization per experiment\n",
        "    for k_value in k_values:\n",
        "        for lambda_value in lambda_values:\n",
        "            experiment_num += 1\n",
        "\n",
        "            # Ensure a clean slate for TensorFlow before each experiment run\n",
        "            tf.compat.v1.reset_default_graph()\n",
        "            gc.collect() # Force garbage collection\n",
        "\n",
        "            print(f\"\\n{'='*20} EXPERIMENT {experiment_num}/{total_experiments} {'='*20}\")\n",
        "            print(f\"Configuration: K={k_value}, Î»={lambda_value}\")\n",
        "\n",
        "            try:\n",
        "                # Run GPU-optimized experiment\n",
        "                result = run_single_experiment(\n",
        "                    k_value=k_value,\n",
        "                    lambda_value=lambda_value,\n",
        "                    epochs=epochs,\n",
        "                    experiment_num=experiment_num,\n",
        "                    total_experiments=total_experiments\n",
        "                )\n",
        "\n",
        "                all_results.append(result)\n",
        "\n",
        "                # Memory cleanup between experiments (already done at start of loop, but extra gc.collect here is fine)\n",
        "                gc.collect()\n",
        "\n",
        "                # Check if the experiment actually completed successfully before trying to access detailed results\n",
        "                if result and result.get(\"status\") == \"Completed\":\n",
        "                    print(f\" Experiment {experiment_num}/{total_experiments} completed\")\n",
        "                    # Safely access keys, providing defaults if they might be missing\n",
        "                    print(f\"   Best NDCG: {result.get('best_ndcg', 0.0):.4f}\")\n",
        "                    print(f\"   GPU Efficiency: {result.get('gpu_efficiency_batches_per_min', 0):.1f} batches/min\")\n",
        "                elif result:\n",
        "                    print(f\" Experiment {experiment_num}/{total_experiments} finished with status: {result.get('status', 'Unknown')}\")\n",
        "                    if result.get('error_message'):\n",
        "                        print(f\"   Error: {result.get('error_message')}\")\n",
        "                else:\n",
        "                    print(f\" Experiment {experiment_num}/{total_experiments} failed to return a result object.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Experiment {experiment_num} failed: {e}\")\n",
        "                # Continue with next experiment\n",
        "                continue\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# Main execution\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    print(f\"\\nðŸ”¬ Starting GPU-Optimized Grid Search on {DATASET.upper()}\")\n",
        "\n",
        "    # Use the optimized parallel grid search\n",
        "    grid_search_results = run_parallel_grid_search(\n",
        "        k_values=HARD_NEG_SAMPLES_K,\n",
        "        lambda_values=CONTRASTIVE_WEIGHTS,\n",
        "        epochs=GRID_SEARCH_EPOCHS,\n",
        "        dataset_name=DATASET\n",
        "    )\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Grid search completed! {len(grid_search_results)} experiments finished.\")\n",
        "\n",
        "    # Save complete grid search summary\n",
        "    save_grid_search_summary_to_drive(grid_search_results, DATASET)\n",
        "\n",
        "    # Final grid search analysis with GPU efficiency metrics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" GPU-OPTIMIZED GRID SEARCH RESULTS ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if grid_search_results:\n",
        "        # Find best configuration\n",
        "        best_result = max(grid_search_results, key=lambda x: x['best_ndcg'])\n",
        "\n",
        "        print(f\" Best Configuration:\")\n",
        "        print(f\"   K={best_result['k_value']}, Î»={best_result['lambda_value']}\")\n",
        "        print(f\"   Experiment Type: {best_result['experiment_type']}\")\n",
        "        print(f\"   Best NDCG: {best_result['best_ndcg']:.4f}\")\n",
        "        print(f\"   Best HR: {best_result['best_hr']:.4f}\")\n",
        "        print(f\"   Best Epoch: {best_result['best_epoch']}\")\n",
        "        print(f\"   Duration: {best_result['duration_minutes']:.1f} minutes\")\n",
        "        print(f\"   GPU Efficiency: {best_result.get('gpu_efficiency_batches_per_min', 0):.1f} batches/min\")\n",
        "\n",
        "        # GPU efficiency analysis\n",
        "        avg_gpu_efficiency = np.mean([r.get('gpu_efficiency_batches_per_min', 0) for r in grid_search_results])\n",
        "        total_duration = sum(r['duration_minutes'] for r in grid_search_results)\n",
        "\n",
        "        # Results table\n",
        "        print(f\"\\n All Results Summary (sorted by NDCG):\")\n",
        "        print(\"=\"*100)\n",
        "        print(f\"{'K':<3} {'Î»':<8} {'Type':<25} {'Best HR':<8} {'Best NDCG':<10} {'Epoch':<6} {'GPU Eff':<8}\")\n",
        "        print(\"-\"*100)\n",
        "\n",
        "        for result in sorted(grid_search_results, key=lambda x: x['best_ndcg'], reverse=True):\n",
        "            exp_type_short = result['experiment_type'][:23] + \"..\" if len(result['experiment_type']) > 25 else result['experiment_type']\n",
        "            gpu_eff = result.get('gpu_efficiency_batches_per_min', 0)\n",
        "            print(f\"{result['k_value']:<3} {result['lambda_value']:<8.3f} {exp_type_short:<25} {result['best_hr']:<8.4f} {result['best_ndcg']:<10.4f} {result['best_epoch']:<6} {gpu_eff:<8.1f}\")\n",
        "\n",
        "        # Analysis insights\n",
        "        print(f\"\\n Analysis Insights:\")\n",
        "\n",
        "        # Best K analysis\n",
        "        k_performance = {}\n",
        "        for k in HARD_NEG_SAMPLES_K:\n",
        "            k_results = [r for r in grid_search_results if r['k_value'] == k]\n",
        "            if k_results:\n",
        "                avg_ndcg = sum(r['best_ndcg'] for r in k_results) / len(k_results)\n",
        "                k_performance[k] = avg_ndcg\n",
        "\n",
        "        if k_performance:\n",
        "            best_k = max(k_performance, key=k_performance.get)\n",
        "            print(f\"   â€¢ Best K value overall: {best_k} (avg NDCG: {k_performance[best_k]:.4f})\")\n",
        "\n",
        "        # Best Î» analysis\n",
        "        lambda_performance = {}\n",
        "        for lam in CONTRASTIVE_WEIGHTS:\n",
        "            lam_results = [r for r in grid_search_results if r['lambda_value'] == lam]\n",
        "            if lam_results:\n",
        "                avg_ndcg = sum(r['best_ndcg'] for r in lam_results) / len(lam_results)\n",
        "                lambda_performance[lam] = avg_ndcg\n",
        "\n",
        "        if lambda_performance:\n",
        "            best_lambda = max(lambda_performance, key=lambda_performance.get)\n",
        "            print(f\"   â€¢ Best Î» value overall: {best_lambda} (avg NDCG: {lambda_performance[best_lambda]:.4f})\")\n",
        "\n",
        "        # Edge case analysis\n",
        "        baseline_result = next((r for r in grid_search_results if r['k_value'] == 0 and r['lambda_value'] == 0), None)\n",
        "        if baseline_result:\n",
        "            print(f\"   â€¢ Pure SelfGNN (K=0, Î»=0): NDCG={baseline_result['best_ndcg']:.4f}\")\n",
        "            improvement = ((best_result['best_ndcg'] - baseline_result['best_ndcg']) / baseline_result['best_ndcg'] * 100)\n",
        "            print(f\"   â€¢ Best configuration improvement over baseline: +{improvement:.2f}%\")\n",
        "\n",
        "        # Component analysis\n",
        "        hard_neg_only = [r for r in grid_search_results if r['k_value'] > 0 and r['lambda_value'] == 0]\n",
        "        contrastive_only = [r for r in grid_search_results if r['k_value'] == 0 and r['lambda_value'] > 0]\n",
        "\n",
        "        if hard_neg_only:\n",
        "            best_hard_neg_only = max(hard_neg_only, key=lambda x: x['best_ndcg'])\n",
        "            print(f\"   â€¢ Best hard negatives only (Î»=0): K={best_hard_neg_only['k_value']}, NDCG={best_hard_neg_only['best_ndcg']:.4f}\")\n",
        "\n",
        "        if contrastive_only:\n",
        "            best_contrastive_only = max(contrastive_only, key=lambda x: x['best_ndcg'])\n",
        "            print(f\"   â€¢ Best contrastive only (K=0): Î»={best_contrastive_only['lambda_value']}, NDCG={best_contrastive_only['best_ndcg']:.4f}\")\n",
        "\n",
        "        print(f\"\\n All results saved to Google Drive at: {DRIVE_RESULTS_PATH}\")\n",
        "    else:\n",
        "        print(\" No successful experiments completed\")\n",
        "\n",
        "else:\n",
        "    # Single experiment mode\n",
        "    print(f\"\\n Running Single Experiment on {DATASET.upper()}\")\n",
        "\n",
        "    result = run_single_experiment(\n",
        "        k_value=SINGLE_K,\n",
        "        lambda_value=SINGLE_LAMBDA,\n",
        "        epochs=SINGLE_EPOCHS\n",
        "    )\n",
        "\n",
        "    print(f\"\\n Single Experiment Summary:\")\n",
        "    print(f\"  â€¢ Best NDCG@10: {result['best_ndcg']:.4f} (Epoch {result['best_epoch']})\")\n",
        "    print(f\"  â€¢ Best HR@10: {result['best_hr']:.4f}\")\n",
        "    print(f\"  â€¢ Final NDCG@10: {result['final_ndcg']:.4f}\")\n",
        "    print(f\"  â€¢ Final HR@10: {result['final_hr']:.4f}\")\n",
        "    print(f\"  â€¢ Duration: {result['duration_minutes']:.1f} minutes\")\n",
        "    print(f\" Result saved to Google Drive at: {DRIVE_RESULTS_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EypVehI6Ao2d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp11hTi8OsY7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.system('kill -9 -1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooOi9k3yOszB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
